{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if GPU is available and if torch is using it ..\n",
      "\n",
      "\n",
      "Torch Cuda is available?\n",
      "True\n",
      "Torch Cuda device count is :\n",
      "1\n",
      "Torch Cuda current device is :\n",
      "0\n",
      "Torch Cuda device is :\n",
      "<torch.cuda.device object at 0x000001F7AE288E50>\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Pytorch version：\n",
      "1.13.0\n",
      "CUDA Version: \n",
      "11.6\n",
      "cuDNN version is :\n",
      "8302\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Packages import\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "from time import strftime\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "from src.cuda_checker import cuda_torch_check, memory_checker\n",
    "\n",
    "### My modules import\n",
    "from src.data_loader import argObj, data_loaders_stimuli_fmri\n",
    "from src import image_preprocessing\n",
    "from src.feature_extraction import model_loader, fit_pca, pca_batch_calculator, extract_and_pca_features, extract_features_no_pca\n",
    "from src.encoding import linear_regression, compute_perason_numpy\n",
    "from src.evaluation_metrics import median_squared_noisenorm_correlation\n",
    "from src.visualize import histogram, box_plot, noise_norm_corr_ROI, final_subj_corr_dataframe_boxplot_istograms\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import pearsonr\n",
    "from src.visualize import histogram, box_plot\n",
    "\n",
    "### Cuda setup and check\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Select the device to run the model on\n",
    "device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n",
    "# Check if cuda is available\n",
    "device = torch.device(device)\n",
    "cuda_torch_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 90 # X% of the training data will be used for training, (100-X)% for validation\n",
    "transform = image_preprocessing.imagenet_V2_transform\n",
    "\n",
    "batch_size = 32\n",
    "pca_component = 1024\n",
    "min_pca_batch_size = pca_component + 300 # pca_component * 2\n",
    "\n",
    "compute_pca = True\n",
    "architecture = \"EndtoEnd\" #@param [\"EndtoEnd\", \"TwoStep\"] {allow-input: true}\n",
    "feature_model_type = \"VGG19\" #@param [\"alexnet\", \"ZFNet\", \"resnet50\", \"vgg16\",\"vgg19_bn\" , \"efficientnetb2\", \"efficientnetb2lib\"]\n",
    "model_layer = \"features.43\"\n",
    "regression_type = \"MLP\" #@param [\"linear\", \"ridge\"]\n",
    "\n",
    "save = False \n",
    "\n",
    "alpha_l = 1e5\n",
    "alpha_r = 1e5\n",
    "grid_search = False\n",
    "\n",
    "### Path definition\n",
    "if isinstance(model_layer, list):\n",
    "    model_layer_full = '+'.join(model_layer)\n",
    "else:\n",
    "    model_layer_full = model_layer\n",
    "\n",
    "datetime_id = strftime(\"(%Y-%m-%d_%H-%M)\")\n",
    "submission_name = f'{strftime(\"(%Y-%m-%d_%H-%M)\")}-{architecture}_{feature_model_type}_{model_layer}-pca_{pca_component}-{regression_type}-alpha_{\"{:.1e}\".format(alpha_l)}'\n",
    "\n",
    "data_home_dir = '../Datasets/Biomedical'\n",
    "data_dir = '../Datasets/Biomedical/algonauts_2023_challenge_data'\n",
    "# Used to save the prediction of saved model\n",
    "parent_submission_dir = f'./files/submissions/{submission_name}'\n",
    "if not os.path.isdir(parent_submission_dir) and save:\n",
    "            os.makedirs(parent_submission_dir)\n",
    "ncsnr_dir = '../Datasets/Biomedical/algonauts_ncsnr'\n",
    "images_trials_dir = '../Datasets/Biomedical/algonauts_train_images_trials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(2023-06-19_15-03)-EndtoEnd_VGG19_features.43-pca_1024-MLP-alpha_1.0e+05'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ Subject: 1 ############################ \n",
      "\n",
      "## Stimulus Images Loading: Info\n",
      "Total train images: 9841\n",
      "Training stimulus images: 8857\n",
      "Validation stimulus images: 984\n",
      "Test stimulus images: 159\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('############################ Subject: ' + str(subj) + ' ############################ \\n')\n",
    "# Definining paths to data and submission directories ##\n",
    "args = argObj(subj, data_home_dir, data_dir, parent_submission_dir, ncsnr_dir, images_trials_dir, save) \n",
    "# Obtain the indices of the training, validation and test data\n",
    "idxs_train, idxs_val, idxs_test, train_imgs_paths, test_imgs_paths = args.images_idx_splitter(train_percentage)\n",
    "\n",
    "# Defining the images data loaderds\n",
    "data_loaders = data_loaders_stimuli_fmri(idxs_train, \n",
    "                                            idxs_val, \n",
    "                                            idxs_test, \n",
    "                                            train_imgs_paths, \n",
    "                                            test_imgs_paths,\n",
    "                                            lh_fmri_path = args.lh_fmri,\n",
    "                                            rh_fmri_path = args.rh_fmri)\n",
    "\n",
    "train_dataloader_lh, val_dataloader_lh, test_dataloader_lh, train_dataloader_rh, val_dataloader_rh, test_dataloader_rh = data_loaders.images_fmri_dataloader(batch_size, transform)\n",
    "\n",
    "lh_fmri_train, lh_fmri_val, rh_fmri_train, rh_fmri_val = data_loaders.fmri_splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Input Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(train_dataloader_lh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19004"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[1].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architettura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione della rete custom\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(CustomNet, self).__init__()\n",
    "        \n",
    "        # Caricamento del modello VGG-19 pre-addestrato\n",
    "        # vgg = models.vgg19_bn(pretrained=True)\n",
    "        \n",
    "        # Utilizziamo solo i layer di feature extraction senza gli ultimi layer completamente connessi\n",
    "        self.features = nn.Sequential(*list(models.alexnet(pretrained=True).features.children()))\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Aggiunta del layer di average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(6, 6))\n",
    "        \n",
    "        # Aggiunta del layer completamente connesso per la regressione\n",
    "        self.fc = nn.Linear(9216, num_outputs)  # L'input size 512 è determinato dal numero di feature maps generate dall'ultimo layer di VGG-19\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giorg\\miniconda3\\envs\\algonauts_test\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giorg\\miniconda3\\envs\\algonauts_test\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=9216, out_features=19004, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Creazione dell'istanza del modello\n",
    "model = CustomNet(first_batch[1].shape[1])\n",
    "model.to(device)\n",
    "model.train()\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "# Stampa della struttura del modello\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.weight, requires_grad: False\n",
      "Layer: features.0.bias, requires_grad: False\n",
      "Layer: features.3.weight, requires_grad: False\n",
      "Layer: features.3.bias, requires_grad: False\n",
      "Layer: features.6.weight, requires_grad: False\n",
      "Layer: features.6.bias, requires_grad: False\n",
      "Layer: features.8.weight, requires_grad: False\n",
      "Layer: features.8.bias, requires_grad: False\n",
      "Layer: features.10.weight, requires_grad: False\n",
      "Layer: features.10.bias, requires_grad: False\n",
      "Layer: fc.weight, requires_grad: True\n",
      "Layer: fc.bias, requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'Layer: {name}, requires_grad: {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.features[-3].weight.grad)\n",
    "print(model.fc.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrica da printare\n",
    "def mean_pearson_corr(y_true, y_pred):\n",
    "    pearson_corr = 0.0\n",
    "    for i in range(y_true.shape[1]):\n",
    "        corr, _ = pearsonr(y_true[:, i], y_pred[:, i])\n",
    "        if corr < 0:\n",
    "            corr = 0.0\n",
    "        if corr > 1:\n",
    "            corr = 1.0\n",
    "        pearson_corr += corr\n",
    "    return pearson_corr / y_true.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametri di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to('cpu') # sposto sulla ram\n",
    "# del model\n",
    "# torch.cuda.empty_cache() # elimino la chache vram\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione della loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Selezionare solo i parametri del layer regressor per l'ottimizzazione\n",
    "# parameters = model.fc.parameters()\n",
    "\n",
    "# Definizione dell'ottimizzatore\n",
    "# optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:36<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.9794, RMSE: 1.3346, MAE: 1.0721, Pearson Corr: 0.1655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:33<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 2.0223, RMSE: 1.3711, MAE: 1.1010, Pearson Corr: 0.2275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:34<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 3.2651, RMSE: 1.7113, MAE: 1.3841, Pearson Corr: 0.2360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:33<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 4.4712, RMSE: 1.9926, MAE: 1.6180, Pearson Corr: 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:34<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 4.5032, RMSE: 1.9741, MAE: 1.6164, Pearson Corr: 0.2681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:32<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 4.3553, RMSE: 1.9231, MAE: 1.5923, Pearson Corr: 0.2949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:35<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 4.9049, RMSE: 2.0222, MAE: 1.6879, Pearson Corr: 0.3013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:33<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 5.5918, RMSE: 2.1434, MAE: 1.7991, Pearson Corr: 0.3013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:34<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 5.7595, RMSE: 2.1586, MAE: 1.8181, Pearson Corr: 0.3113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [04:35<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 5.4621, RMSE: 2.0880, MAE: 1.7635, Pearson Corr: 0.3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #model.train()\n",
    "    total_loss = 0.0\n",
    "    total_rmse = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_pearson_corr = 0.0\n",
    "    total_batches = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_dataloader_lh):\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        # Reset dei gradienti\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Passaggio degli input attraverso la rete\n",
    "        outputs = model(images.to(device))\n",
    "        \n",
    "        # Calcolo della loss\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clean memory\n",
    "        images = images.detach().cpu()\n",
    "        labels = labels.detach().cpu()\n",
    "        outputs = outputs.detach().cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Ottimizzazione dei parametri\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calcolo delle metriche di accuratezza\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_rmse += mean_squared_error(labels.detach().cpu().numpy(), outputs.detach().cpu().numpy(), squared=False) * batch_size\n",
    "        total_mae += mean_absolute_error(labels.detach().cpu().numpy(), outputs.detach().cpu().numpy()) * batch_size\n",
    "        total_pearson_corr += mean_pearson_corr(labels.detach().cpu().numpy(), outputs.detach().cpu().numpy()) * batch_size\n",
    "        total_batches += batch_size\n",
    "    \n",
    "    # Calcolo delle metriche di accuratezza medie\n",
    "    average_loss = total_loss / total_batches\n",
    "    average_rmse = total_rmse / total_batches\n",
    "    average_mae = total_mae / total_batches\n",
    "    average_pearson_corr = total_pearson_corr / total_batches\n",
    "    \n",
    "    # Stampa delle metriche di accuratezza\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}, RMSE: {average_rmse:.4f}, MAE: {average_mae:.4f}, Pearson Corr: {average_pearson_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=9216, out_features=19004, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader_lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "lh_fmri_val_pred = None\n",
    "lh_fmri_val_ground = None\n",
    "# Initialize counter for stacked batches\n",
    "count_stacked_batches = 0\n",
    "# Define list to store downsampled features\n",
    "features = []\n",
    "\n",
    "for _, (images, labels) in tqdm(enumerate(val_dataloader_lh), total=len(val_dataloader_lh)):\n",
    "    if lh_fmri_val_pred is None and lh_fmri_val_ground is None:\n",
    "        with torch.no_grad():\n",
    "            lh_fmri_val_pred = model(images.to(device)).detach().cpu().numpy()\n",
    "        lh_fmri_val_ground = labels.numpy()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            lh_fmri_val_pred = np.vstack((lh_fmri_val_pred, model(images.to(device)).detach().cpu().numpy()))\n",
    "        lh_fmri_val_ground = np.vstack((lh_fmri_val_ground, labels.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:10<00:00,  6.11it/s]\n"
     ]
    }
   ],
   "source": [
    "rh_fmri_val_pred = None\n",
    "# Initialize counter for stacked batches\n",
    "count_stacked_batches = 0\n",
    "# Define list to store downsampled features\n",
    "features = []\n",
    "\n",
    "for _, (images, labels) in tqdm(enumerate(val_dataloader_rh), total=len(val_dataloader_rh)):\n",
    "    if rh_fmri_val_pred is None:\n",
    "        rh_fmri_val_pred = model(images.to(device)).detach().cpu().numpy()\n",
    "    else:\n",
    "        rh_fmri_val_pred = np.vstack((rh_fmri_val_pred, model(images.to(device)).detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_fmri_val_pred = rh_fmri_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the correlation between the predicted and actual fMRI data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19004/19004 [00:01<00:00, 12708.69it/s]\n",
      "100%|██████████| 20544/20544 [00:01<00:00, 16406.46it/s]\n"
     ]
    }
   ],
   "source": [
    "noise_norm_corr_dict_1, noise_norm_corr_dict_2 = median_squared_noisenorm_correlation(lh_fmri_val_pred, \n",
    "                                                                                        rh_fmri_val_pred,\n",
    "                                                                                        lh_fmri_val,\n",
    "                                                                                        rh_fmri_val,\n",
    "                                                                                        args.data_dir,\n",
    "                                                                                        args.ncsnr_dir,\n",
    "                                                                                        args.images_trials_dir,\n",
    "                                                                                        idxs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030948432878404552"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(noise_norm_corr_dict_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolato con la funzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025723810331753576"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pearson_corr(lh_fmri_val, lh_fmri_val_pred)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_norm_corr_ROI_dict_1 = noise_norm_corr_ROI(args.data_dir, \n",
    "                                                                  noise_norm_corr_dict_1, \n",
    "                                                                  noise_norm_corr_dict_2,\n",
    "                                                                  save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(984, 19004)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_fmri_val_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(984, 19004)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_fmri_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 19004])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(first_batch[0].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, feature_extractor = model_loader(feature_model_type, model_layer, device)\n",
    "\n",
    "# Fit the PCA model\n",
    "if compute_pca:\n",
    "    # Fit the PCA model\n",
    "    pca_batch_size, n_stacked_batches = pca_batch_calculator(len(idxs_train),\n",
    "                                                            batch_size,\n",
    "                                                            min_pca_batch_size,\n",
    "                                                            pca_component)\n",
    "    \n",
    "    pca = fit_pca(feature_extractor,\n",
    "                    train_imgs_dataloader,\n",
    "                    pca_component,\n",
    "                    n_stacked_batches,\n",
    "                    pca_batch_size,\n",
    "                    device)\n",
    "    print(\"Comulative Explained variance ratio: \", sum(pca.explained_variance_ratio_))\n",
    "    print(\"Number of components: \", pca.n_components_)\n",
    "    \n",
    "    print('## Extracting features from training, validation and test data...')\n",
    "    features_train = extract_and_pca_features(feature_extractor, train_imgs_dataloader, pca, n_stacked_batches, device)\n",
    "    features_val = extract_and_pca_features(feature_extractor, val_imgs_dataloader, pca, n_stacked_batches, device)\n",
    "    features_test = extract_and_pca_features(feature_extractor, test_imgs_dataloader, pca, n_stacked_batches, device)\n",
    "    \n",
    "    # print(\"\\n\")\n",
    "    # print('## Checking and Freeing  GPU memory...')\n",
    "    # memory_checker()\n",
    "    model.to('cpu') # sposto sulla ram\n",
    "    feature_extractor.to('cpu') # sposto sulla ram\n",
    "    del model, feature_extractor, pca, train_imgs_dataloader, val_imgs_dataloader, test_imgs_dataloader  # elimino dalla ram\n",
    "    torch.cuda.empty_cache() # elimino la chache vram\n",
    "    gc.collect() # elimino la cache ram\n",
    "    # memory_checker()\n",
    "else:\n",
    "    print('## Extracting features from training, validation and test data...')\n",
    "    features_train = extract_features_no_pca(feature_extractor, train_imgs_dataloader, device)\n",
    "    features_val = extract_features_no_pca(feature_extractor, val_imgs_dataloader, device)\n",
    "    features_test = extract_features_no_pca(feature_extractor, test_imgs_dataloader, device)\n",
    "    \n",
    "    model.to('cpu') # sposto sulla ram\n",
    "    feature_extractor.to('cpu') # sposto sulla ram\n",
    "    del model, feature_extractor, train_imgs_dataloader, val_imgs_dataloader, test_imgs_dataloader  # elimino dalla ram\n",
    "    torch.cuda.empty_cache() # elimino la chache vram\n",
    "    gc.collect() # elimino la cache ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_fmri_train, lh_fmri_val, rh_fmri_train, rh_fmri_val = data_loaders.fmri_splitter()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ## Fit Encoder and Predict...\n",
      "LH fMRI number of vertices: (8857, 19004)\n",
      "RH fMRI number of vertices: (8857, 20544)\n"
     ]
    }
   ],
   "source": [
    "## Fit the linear model ##\n",
    "print('\\n ## Fit Encoder and Predict...')\n",
    "lh_fmri_train, lh_fmri_val, rh_fmri_train, rh_fmri_val = data_loaders.fmri_splitter()\n",
    "print('LH fMRI number of vertices:', lh_fmri_train.shape)\n",
    "print('RH fMRI number of vertices:', rh_fmri_train.shape)\n",
    "# param_grid = {'alpha': [0.0001, 0.0002, 0.001, 0.01, 0.1, 1, 10, 100, 1e3, 5e3, 1e4, 2e4, 5e4, 1e5, 1e6]}\n",
    "\n",
    "param_grid = {'alpha': [1, 10, 100, 1e3, 5e3, 1e4, 2e4, 5e4, 1e5, 2e5, 5e5, 1e6]}\n",
    "#param_grid = {'alpha': [1e6, 2e6, 5e6, 1e7, 2e7, 5e7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Param: {'alpha': 100000.0}\n",
      "Best Score: 0.4171876543623486\n"
     ]
    }
   ],
   "source": [
    "grid_search_l = GridSearchCV(Ridge(), param_grid=param_grid, scoring=make_scorer(\n",
    "    lambda x, y: np.median(compute_perason_numpy(x, y))), cv=5, n_jobs=5, verbose=1)\n",
    "grid_search_l.fit(X=features_train, y=lh_fmri_train)\n",
    "print(\"Best Param: {}\".format(grid_search_l.best_params_))\n",
    "print(\"Best Score: {}\".format(grid_search_l.best_score_))\n",
    "alpha_l = grid_search_l.best_params_['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_r = alpha_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000.0\n"
     ]
    }
   ],
   "source": [
    "print(alpha_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "grid_search_r = GridSearchCV(Ridge(), param_grid=param_grid, scoring=make_scorer(\n",
    "    lambda x, y: np.median(compute_perason_numpy(x, y))), cv=5, n_jobs=5, verbose=1)\n",
    "grid_search_r.fit(X=features_train, y=rh_fmri_train)\n",
    "print(\"Best Param: {}\".format(grid_search_r.best_params_))\n",
    "print(\"Best Score: {}\".format(grid_search_r.best_score_))\n",
    "alpha_r = grid_search_r.best_params_['alpha']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ridge regressions on the training data...\n",
      "Predicting fMRI data on the validation and test data...\n",
      "Computing the correlation between the predicted and actual fMRI data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19004/19004 [00:01<00:00, 9556.57it/s] \n",
      "100%|██████████| 20544/20544 [00:01<00:00, 11662.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score -> Median Noise Normalized Squared Correlation Percentage (LH and RH)\n",
      "LH subj 1 | Score:  48.39405012388089\n",
      "RH subj 1 | Score:  47.737441465482924\n"
     ]
    }
   ],
   "source": [
    "lh_fmri_val_pred, lh_fmri_test_pred, rh_fmri_val_pred, rh_fmri_test_pred = linear_regression(regression_type, \n",
    "                                                                                                features_train, \n",
    "                                                                                                features_val, \n",
    "                                                                                                features_test, \n",
    "                                                                                                lh_fmri_train, \n",
    "                                                                                                rh_fmri_train, \n",
    "                                                                                                save_predictions,\n",
    "                                                                                                args.subject_test_submission_dir,\n",
    "                                                                                                alpha_l,\n",
    "                                                                                                alpha_r,\n",
    "                                                                                                grid_search= False)\n",
    "\n",
    "noise_norm_corr_dict[f'lh_{subj}'], noise_norm_corr_dict[f'rh_{subj}'] = median_squared_noisenorm_correlation(lh_fmri_val_pred, \n",
    "                                                                                                                rh_fmri_val_pred,\n",
    "                                                                                                                lh_fmri_val,\n",
    "                                                                                                                rh_fmri_val,\n",
    "                                                                                                                args.data_dir,\n",
    "                                                                                                                args.ncsnr_dir,\n",
    "                                                                                                                args.images_trials_dir,\n",
    "                                                                                                                idxs_val)\n",
    "print(\"\\n Score -> Median Noise Normalized Squared Correlation Percentage (LH and RH)\")\n",
    "print(\"LH subj\",subj,\"| Score: \",np.median(noise_norm_corr_dict[f'lh_{subj}'])*100)\n",
    "print(\"RH subj\",subj,\"| Score: \",np.median(noise_norm_corr_dict[f'rh_{subj}'])*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not bool",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m histogram(args\u001b[39m.\u001b[39;49mdata_dir, noise_norm_corr_dict[\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlh_\u001b[39;49m\u001b[39m{\u001b[39;49;00msubj\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m      2\u001b[0m           noise_norm_corr_dict[\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrh_\u001b[39;49m\u001b[39m{\u001b[39;49;00msubj\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m      3\u001b[0m           submission_name, \n\u001b[0;32m      4\u001b[0m           save \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\Projects\\Thesis\\src\\visualize.py:91\u001b[0m, in \u001b[0;36mhistogram\u001b[1;34m(roi_path, lh_correlation, rh_correlation, title, save)\u001b[0m\n\u001b[0;32m     84\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(title_text\u001b[39m=\u001b[39mtitle, yaxis\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mrange\u001b[39m\u001b[39m=\u001b[39m[\u001b[39m0.0\u001b[39m, \u001b[39m1.0\u001b[39m]), yaxis_title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMedian Noise Normalized Encoding Accuracy\u001b[39m\u001b[39m\"\u001b[39m, xaxis_title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mROIs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m save \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m \n\u001b[0;32m     88\u001b[0m     \u001b[39m# if not osp.isdir(save):\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     \u001b[39m#     os.makedirs(save)\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m     to_save \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39;49mjoin(save, \u001b[39m\"\u001b[39;49m\u001b[39mhistogram_pearson_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\n\u001b[0;32m     92\u001b[0m         strftime(\u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m%\u001b[39;49m\u001b[39mH\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n\u001b[0;32m     94\u001b[0m     fig\u001b[39m.\u001b[39mwrite_html(to_save\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.html\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m     fig\u001b[39m.\u001b[39mwrite_image(to_save\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\giorg\\miniconda3\\envs\\algonauts\\lib\\ntpath.py:78\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(path, \u001b[39m*\u001b[39mpaths):\n\u001b[1;32m---> 78\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mfspath(path)\n\u001b[0;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m     80\u001b[0m         sep \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not bool"
     ]
    }
   ],
   "source": [
    "histogram(args.data_dir, noise_norm_corr_dict[f'lh_{subj}'], \n",
    "          noise_norm_corr_dict[f'rh_{subj}'], \n",
    "          submission_name, \n",
    "          save = args.subject_images_submission_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot(args.data_dir, noise_norm_corr_dict[f'lh_{subj}'], \n",
    "          noise_norm_corr_dict[f'rh_{subj}'], \n",
    "          submission_name, \n",
    "          save = args.subject_images_submission_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algonauts_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
