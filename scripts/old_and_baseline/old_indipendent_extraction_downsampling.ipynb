{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages \n",
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision.models import AlexNet_Weights, VGG16_Weights, VGG16_BN_Weights, VGG19_Weights, EfficientNet_B2_Weights\n",
    "from torchvision import transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from scipy.stats import pearsonr as corr\n",
    "\n",
    "# Functions and classes\n",
    "# Platform definition\n",
    "platform = 'jupyter_notebook'\n",
    "if platform == 'jupyter_notebook':\n",
    "    # data_dir = '../../../Projects/Datasets/Biomedical/algonauts_2023_challenge_data'\n",
    "    # Data folder definition\n",
    "    data_dir = '../Datasets/Biomedical/algonauts_2023_challenge_data'\n",
    "    # Used to save the prediction of saved model\n",
    "    parent_submission_dir = './algonauts_2023_challenge_submission'\n",
    "    ncsnr_dir = '../Datasets/Biomedical/algonauts_ncsnr'\n",
    "    images_trials = '../Datasets/Biomedical/algonauts_train_images_trials'\n",
    "        \n",
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj, parent_ncsnr_dir = ncsnr_dir, images_trials_parent_dir = images_trials):\n",
    "    # Define the dir where data is stored\n",
    "    # 1 became 01\n",
    "    self.subj = format(subj, '02') # '0numberofchars'\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "\n",
    "    # NCSNR\n",
    "    self.parent_ncsnr_dir = parent_ncsnr_dir\n",
    "    self.ncsnr_dir = os.path.join(self.parent_ncsnr_dir, 'subj'+self.subj)\n",
    "    \n",
    "    # SUBMISSION DIR\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "    # Train Images Trials \n",
    "    self.images_trials_parent_dir = images_trials_parent_dir\n",
    "    self.images_trials_dir = os.path.join(self.images_trials_parent_dir, 'subj'+self.subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if GPU is available and if torch is using it ..\n",
      "\n",
      "\n",
      "Torch Cuda is available?\n",
      "True\n",
      "Torch Cuda device count is :\n",
      "1\n",
      "Torch Cuda current device is :\n",
      "0\n",
      "Torch Cuda device is :\n",
      "<torch.cuda.device object at 0x000001A46ECB1430>\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Pytorch version：\n",
      "1.13.0\n",
      "CUDA Version: \n",
      "11.6\n",
      "cuDNN version is :\n",
      "8302\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Parameters ###\n",
    "pca_component = 300\n",
    "train_percentage = 90 # X% of the training data will be used for training, (100-X)% for validation\n",
    "batch_mode = \"static\" # \"dynamic\" or \"static\"\n",
    "batch_size_min = 40 # Batch size has to be major than pca_component\n",
    "batch_size_max = 100\n",
    "batch_size = 200\n",
    "save_predictions = True \n",
    "feature_model_type = \"alexnet\" #@param [\"alexnet\", \"vgg16\", \"efficientnetb2\", \"efficientnetb2lib\"]\n",
    "regression_type = \"linear\" #@param [\"linear\", \"ridge\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "            \n",
    "    # Select the device to run the model on\n",
    "    device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    # Check if GPU is available and torch is using it\n",
    "    print(\"Check if GPU is available and if torch is using it ..\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Torch Cuda is available?\")\n",
    "    print(torch.cuda.is_available())\n",
    "    print(\"Torch Cuda device count is :\")\n",
    "    print(torch.cuda.device_count())\n",
    "    print(\"Torch Cuda current device is :\")\n",
    "    print(torch.cuda.current_device())\n",
    "    print(\"Torch Cuda device is :\")\n",
    "    print(torch.cuda.device(0))\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Pytorch version：\")\n",
    "    print(torch.__version__)\n",
    "    print(\"CUDA Version: \")\n",
    "    print(torch.version.cuda)\n",
    "    print(\"cuDNN version is :\")\n",
    "    print(torch.backends.cudnn.version())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape: (9841, 19004)\n",
      "RH training fMRI data shape: (9841, 20544)\n",
      "Total train images: 9841\n",
      "Training stimulus images: 8857\n",
      "Validation stimulus images: 984\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "subj = 1\n",
    "args = argObj(data_dir, parent_submission_dir, subj)\n",
    "        \n",
    "## Load the fmri response for left and right hemispheres ##\n",
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "# Check the shapes of the data\n",
    "print('LH training fMRI data shape:', lh_fmri.shape)\n",
    "print('RH training fMRI data shape:', rh_fmri.shape)\n",
    "\n",
    "## load the stimulus images ##\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "        \n",
    "## Create the training, validation and test partitions indices ##\n",
    "rand_seed = 5 #@param\n",
    "np.random.seed(rand_seed)\n",
    "# train_percentage = 90\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * train_percentage))\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "print('Total train images: ' + str(len(train_img_list)))\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('Validation stimulus images: ' + format(len(idxs_val)))\n",
    "print('Test stimulus images: ' + format(len(idxs_test)))\n",
    "\n",
    "## Create the training, validation and test partitions dataloaders ##\n",
    "# Preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)), # resize the images to 224x224 pixels (256x256)\n",
    "    transforms.CenterCrop(224), # IN THE OLDER SCRIPT: no center crop, only resize to 224x224\n",
    "    transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    # normalize the images color channels\n",
    "    # mean: [0.485, 0.456, 0.406] for the three channels\n",
    "    # std: [0.229, 0.224, 0.225] for the three channels\n",
    "])\n",
    "# Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "# Dataloader class\n",
    "# 3 6 -> 310 / other ones -> 300\n",
    "# if batch_mode == 'static':\n",
    "#     if subj == 3 or subj == 6:\n",
    "#         batch_size = 310\n",
    "#     else: \n",
    "#         batch_size = 300 # 300 #@param (310 for 3 and 6)\n",
    "#     print(batch_mode, \"batch size: \", batch_size)\n",
    "# if batch_mode == 'dynamic':\n",
    "#     # Batch size should never be less than pca components\n",
    "#     for batch_size in range(batch_size_min, batch_size_max + 1):\n",
    "#         if len(idxs_train) % batch_size >= batch_size_min:\n",
    "#             # print(\"Train images: \", len(train_img_list))\n",
    "#             # print(\"Remainder: \", len(train_img_list) % batch_size)\n",
    "#             print(batch_mode, \"Batch size: \", batch_size)\n",
    "#             break\n",
    "\n",
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_train, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_val, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "## Spli the fmri data into training and validation partitions ##\n",
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]\n",
    "del lh_fmri, rh_fmri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\giorg/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "if feature_model_type == 'alexnet':\n",
    "    ## AlexNet ##\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', \n",
    "                        'alexnet', \n",
    "                        weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "    model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "    model.eval() # set the model to evaluation mode, since you are not training it\n",
    "    train_nodes, _ = get_graph_node_names(model)\n",
    "    model_layer = \"features.12\" #@param [\"features.2\", \"features.5\", \"features.7\", \"features.9\", \"features.12\", \"classifier.2\", \"classifier.5\", \"classifier.6\"] {allow-input: true}\n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "elif feature_model_type == 'vgg16':\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', \n",
    "                        'vgg16_bn', \n",
    "                        weights=VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "    model.eval() # set the model to evaluation mode, since you are not training it\n",
    "    model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "    model_layer = \"flatten\" #@param [\"features.2\", \"features.5\", \"features.7\", \"features.10\", \"features.12\", \"features.14\", \"features.17\", \"features.19\", \"features.21\", \"features.24\", \"features.26\", \"features.28\", \"features.31\", \"features.33\", \"features.35\", \"classifier.2\", \"classifier.5\", \"classifier.7\"] {allow-input: true}\n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])\n",
    "    # Freeze the parameters of the feature extractor\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "elif feature_model_type == 'efficientnetb2':\n",
    "    model = models.efficientnet_b2(weights = EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
    "    model.eval() # set the model to evaluation mode, since you are not training it\n",
    "    model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "    model_layer = \"features.8\" #@param ['x', 'features.0', 'features.1.0.block.0', 'features.1.0.block.1', 'features.1.0.block.2', 'features.1.1.block.0', 'features.1.1.block.1', 'features.1.1.block.2', 'features.1.1.stochastic_depth', 'features.1.1.add', 'features.2.0.block.0', 'features.2.0.block.1', 'features.2.0.block.2', 'features.2.0.block.3', 'features.2.1.block.0', 'features.2.1.block.1', 'features.2.1.block.2', 'features.2.1.block.3', 'features.2.1.stochastic_depth', 'features.2.1.add', 'features.2.2.block.0', 'features.2.2.block.1', 'features.2.2.block.2', 'features.2.2.block.3', 'features.2.2.stochastic_depth', 'features.2.2.add', 'features.3.0.block.0', 'features.3.0.block.1', 'features.3.0.block.2', 'features.3.0.block.3', 'features.3.1.block.0', 'features.3.1.block.1', 'features.3.1.block.2', 'features.3.1.block.3', 'features.3.1.stochastic_depth', 'features.3.1.add', 'features.3.2.block.0', 'features.3.2.block.1', 'features.3.2.block.2', 'features.3.2.block.3', 'features.3.2.stochastic_depth', 'features.3.2.add', 'features.4.0.block.0', 'features.4.0.block.1', 'features.4.0.block.2', 'features.4.0.block.3', 'features.4.1.block.0', 'features.4.1.block.1', 'features.4.1.block.2', 'features.4.1.block.3', 'features.4.1.stochastic_depth', 'features.4.1.add', 'features.4.2.block.0', 'features.4.2.block.1', 'features.4.2.block.2', 'features.4.2.block.3', 'features.4.2.stochastic_depth', 'features.4.2.add', 'features.4.3.block.0', 'features.4.3.block.1', 'features.4.3.block.2', 'features.4.3.block.3', 'features.4.3.stochastic_depth', 'features.4.3.add', 'features.5.0.block.0', 'features.5.0.block.1', 'features.5.0.block.2', 'features.5.0.block.3', 'features.5.1.block.0', 'features.5.1.block.1', 'features.5.1.block.2', 'features.5.1.block.3', 'features.5.1.stochastic_depth', 'features.5.1.add', 'features.5.2.block.0', 'features.5.2.block.1', 'features.5.2.block.2', 'features.5.2.block.3', 'features.5.2.stochastic_depth', 'features.5.2.add', 'features.5.3.block.0', 'features.5.3.block.1', 'features.5.3.block.2', 'features.5.3.block.3', 'features.5.3.stochastic_depth', 'features.5.3.add', 'features.6.0.block.0', 'features.6.0.block.1', 'features.6.0.block.2', 'features.6.0.block.3', 'features.6.1.block.0', 'features.6.1.block.1', 'features.6.1.block.2', 'features.6.1.block.3', 'features.6.1.stochastic_depth', 'features.6.1.add', 'features.6.2.block.0', 'features.6.2.block.1', 'features.6.2.block.2', 'features.6.2.block.3', 'features.6.2.stochastic_depth', 'features.6.2.add', 'features.6.3.block.0', 'features.6.3.block.1', 'features.6.3.block.2', 'features.6.3.block.3', 'features.6.3.stochastic_depth', 'features.6.3.add', 'features.6.4.block.0', 'features.6.4.block.1', 'features.6.4.block.2', 'features.6.4.block.3', 'features.6.4.stochastic_depth', 'features.6.4.add', 'features.7.0.block.0', 'features.7.0.block.1', 'features.7.0.block.2', 'features.7.0.block.3', 'features.7.1.block.0', 'features.7.1.block.1', 'features.7.1.block.2', 'features.7.1.block.3', 'features.7.1.stochastic_depth', 'features.7.1.add', 'features.8', 'avgpool', 'flatten', 'classifier.0', 'classifier.1']\n",
    "    feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(f'Feature extractor: {feature_model_type}, layer: {model_layer}')\n",
    "elif feature_model_type == 'efficientnetb2lib':\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b2')\n",
    "    model = nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    def feature_extractor(x):\n",
    "        with torch.no_grad():\n",
    "            x = model(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'features.31', 'features.32', 'features.33', 'features.34', 'features.35', 'features.36', 'features.37', 'features.38', 'features.39', 'features.40', 'features.41', 'features.42', 'features.43', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n"
     ]
    }
   ],
   "source": [
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print(train_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244797440\n",
      "257949696\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244797440\n",
      "257949696\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello va prima spostato alla cpu prima di essere eliminato (pulizia gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to('cpu')\n",
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pulizia memoria ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental PCA + Feature Extraction Fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old One (batch_size = pca_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA ##\n",
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=pca_component, batch_size=batch_size)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        # Fit PCA to batch\n",
    "        for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            # Extract features\n",
    "            ft = feature_extractor(d.to(device))\n",
    "            # Flatten the features\n",
    "            ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "            # Fit PCA to batch\n",
    "            print(ft.shape)\n",
    "            pca.partial_fit(ft.detach().cpu().numpy())\n",
    "            torch.cuda.empty_cache()\n",
    "        return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from training, validation and test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [01:58<00:00,  2.64s/it]\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.52s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape:  (8857, 9216)\n",
      "Features shape:  (984, 9216)\n",
      "Features shape:  (159, 9216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Feature extraction ##\n",
    "def extract_features(feature_extractor, dataloader):\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d.to(device))\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        # print(ft.shape)\n",
    "        features.append(ft.detach().cpu().numpy())\n",
    "    return np.vstack(features)\n",
    "    \n",
    "print('Extracting features from training, validation and test data...')\n",
    "features_train = extract_features(feature_extractor, train_imgs_dataloader)\n",
    "features_val = extract_features(feature_extractor, val_imgs_dataloader)\n",
    "features_test = extract_features(feature_extractor, test_imgs_dataloader)\n",
    "print('Features shape: ', features_train.shape)\n",
    "print('Features shape: ', features_val.shape)\n",
    "print('Features shape: ', features_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New One (batch_size != pca_batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Batch Size calculator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size calculator:\n",
    "Code that allows you to find the value of the variable 'n_stacked_batches' given that:\n",
    "\n",
    "- The variables total_instances (e.g. equal to 9000), batch_size (e.g. equal to 64), and pca_component (e.g. equal to 200) are known.\n",
    "- min_pca_batch_size = pca_component * 2\n",
    "- It is known that pca_batch_size = batch_size * n_stacked_batches\n",
    "It find the value of n_stacked_batches so that:\n",
    "1. total_instances % pca_batch_size >= min_pca_batch_size\n",
    "2. n_stacked_batches must be the smallest available value that satisfies this inequality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches size: 64\n",
      "Total instances: 8431\n",
      "PCA components: 500\n",
      "Minimum pca batch size: 1000\n",
      "Number of stacked batches for pca: 19\n",
      "PCA batch size (batch_size * n_stacked_batches): 1216\n",
      "Last pca batch size: 1135\n"
     ]
    }
   ],
   "source": [
    "total_instances = 8431 # len(idxs_train)\n",
    "batch_size = 64\n",
    "pca_component = 500\n",
    "\n",
    "min_pca_batch_size = pca_component * 2\n",
    "\n",
    "n_stacked_batches = 1\n",
    "while True:\n",
    "    pca_batch_size = batch_size * n_stacked_batches\n",
    "    if total_instances % pca_batch_size >= min_pca_batch_size:\n",
    "        break\n",
    "    n_stacked_batches += 1\n",
    "pca_batch_size = batch_size * n_stacked_batches\n",
    "\n",
    "print(f'Batches size: {batch_size}')\n",
    "print(f'Total instances: {total_instances}')\n",
    "print(f'PCA components: {pca_component}')\n",
    "print(f'Minimum pca batch size: {min_pca_batch_size}')\n",
    "print(f'Number of stacked batches for pca: {n_stacked_batches}')\n",
    "print(f'PCA batch size (batch_size * n_stacked_batches): {pca_batch_size}')\n",
    "print(f'Last pca batch size: {total_instances % pca_batch_size}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor + Incremental PCA FIT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature extractor + pca fit function is able to:\n",
    "1. extract features batches from images batches, obtaining 2d numpy matrices (batch_size x n_features) \n",
    "2. stack n_stacked_batches feature batches vertically\n",
    "3. apply pca partial fit on stacked features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# Initialize empty ft_stacked matrix\n",
    "ft_stacked = None\n",
    "# Initialize counter for stacked batches\n",
    "count_stacked_batches = 0\n",
    "n = 122\n",
    "array_di_batchsize = np.full((n), 64).reshape(n)\n",
    "\n",
    "for _, d in enumerate(array_di_batchsize):\n",
    "    print(_)\n",
    "    # Stack vertically the ft matrix\n",
    "    if ft_stacked is None:\n",
    "        print(f\"Inizio stacking bacth 1 di {n_stacked_batches}\")\n",
    "        ft_stacked = 1\n",
    "    else:\n",
    "        print(\"Stacking batch numero: \", count_stacked_batches + 1, \" di \", n_stacked_batches)\n",
    "        ft_stacked = 1\n",
    "        \n",
    "    # Check if n_stacked_batches is reached\n",
    "    count_stacked_batches += 1\n",
    "    if count_stacked_batches == n_stacked_batches:\n",
    "        # Do something with ft_stacked (e.g., save to disk)\n",
    "        # ...\n",
    "\n",
    "        # Reset ft_stacked and counter\n",
    "        print(f\"Resetto ft_stacked e count_stacked_batches a 0\")\n",
    "        ft_stacked = None\n",
    "        count_stacked_batches = 0\n",
    "\n",
    "# Check if there are remaining stacked batches to process\n",
    "if count_stacked_batches > 0:\n",
    "    # Do something with ft_stacked (e.g., save to disk)\n",
    "    # ...\n",
    "    print(f\"Ultimo batch di ft_stacked incompleto con {count_stacked_batches} batch su {n_stacked_batches}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final PCA Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(feature_extractor, dataloader, pca_component, pca_batch_size):\n",
    "    # Initialize empty ft_stacked matrix which will be used to stack the features \n",
    "    # of n_stacked_batches batches\n",
    "    ft_stacked = None\n",
    "    # Initialize counter for stacked batches\n",
    "    count_stacked_batches = 0\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=pca_component, batch_size=pca_batch_size)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        # Fit PCA to batch\n",
    "        for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            # Bove batch to gpu and extract features\n",
    "            ft = feature_extractor(d.to(device))\n",
    "            # Flatten the features and detach them from the graph\n",
    "            ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()]).detach().cpu().numpy()\n",
    "            # Fit PCA to batch\n",
    "            print(ft.shape)\n",
    "            # Stack vertically the ft matrix\n",
    "            if ft_stacked is None:\n",
    "                # If the feature batch is the first one, initialize ft_stacked\n",
    "                ft_stacked = ft\n",
    "            else:\n",
    "                # if the feature batch is not the first one, stack vertically the ft matrix\n",
    "                ft_stacked = np.vstack((ft_stacked, ft))\n",
    "            # Check if n_stacked_batches is reached\n",
    "            count_stacked_batches += 1\n",
    "            if count_stacked_batches == n_stacked_batches:\n",
    "                print(ft_stacked.shape)\n",
    "                # If n_stacked_batches is reached, fit PCA to ft_stacked\n",
    "                pca.partial_fit(ft_stacked)\n",
    "                # After fitting PCA, reset ft_stacked and counter\n",
    "                ft_stacked = None\n",
    "                count_stacked_batches = 0\n",
    "            # Free VRAM memory by deleting model graph\n",
    "            torch.cuda.empty_cache()\n",
    "        # Check if there are remaining stacked batches to process\n",
    "        # that will be the case if the total number of batches is not a multiple of n_stacked_batches\n",
    "        # the last pca batch will be smaller than pca_batch_size\n",
    "        if count_stacked_batches > 0:\n",
    "            # Fit PCA to ft_stacked\n",
    "            pca.partial_fit(ft_stacked)\n",
    "            torch.cuda.empty_cache()\n",
    "        return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit PCA to training data ##\n",
    "print(f'Fitting Incremental PCA ({pca_component} components) to training data...')\n",
    "pca = fit_pca(feature_extractor, train_imgs_dataloader)\n",
    "print(\"Comulative Explained variance ratio: \", sum(pca.explained_variance_ratio_))\n",
    "print(\"Number of components: \", pca.n_components_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final feature extraction function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(feature_extractor, dataloader, pca):\n",
    "    # Initialize empty ft_stacked matrix which will be used to stack the features \n",
    "    # of n_stacked_batches batches\n",
    "    ft_stacked = None\n",
    "    # Initialize counter for stacked batches\n",
    "    count_stacked_batches = 0\n",
    "    # Define list to store downsampled features\n",
    "    features = []\n",
    "\n",
    "    # Set model to evaluation mode (e.g., disable dropout)\n",
    "    with torch.no_grad(): \n",
    "        for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            # Bove batch to gpu and extract features\n",
    "            ft = feature_extractor(d.to(device))\n",
    "            # Flatten the features and detach them from the graph\n",
    "            ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()]).detach().cpu().numpy()\n",
    "            # Fit PCA to batch\n",
    "            print(ft.shape)\n",
    "            # Stack vertically the ft matrix\n",
    "            if ft_stacked is None:\n",
    "                # If the feature batch is the first one, initialize ft_stacked\n",
    "                ft_stacked = ft\n",
    "            else:\n",
    "                # if the feature batch is not the first one, stack vertically the ft matrix\n",
    "                ft_stacked = np.vstack((ft_stacked, ft))\n",
    "            # Check if n_stacked_batches is reached\n",
    "            count_stacked_batches += 1\n",
    "            if count_stacked_batches == n_stacked_batches:\n",
    "                print(ft_stacked.shape)\n",
    "                # If n_stacked_batches is reached, transform ft_stacked with PCA\n",
    "                ft_stacked = pca.transform(ft_stacked)\n",
    "                # Append downsampled features to features list\n",
    "                features.append(ft_stacked)\n",
    "                # After transforming with PCA, reset ft_stacked and counter\n",
    "                ft_stacked = None\n",
    "                count_stacked_batches = 0\n",
    "            # Free VRAM memory by deleting model graph\n",
    "            torch.cuda.empty_cache()\n",
    "        # Check if there are remaining stacked batches to process\n",
    "        # that will be the case if the total number of batches is not a multiple of n_stacked_batches\n",
    "        # the last pca batch will be smaller than pca_batch_size\n",
    "        if count_stacked_batches > 0:\n",
    "            # Fit PCA to ft_stacked\n",
    "            ft_stacked = pca.transform(ft_stacked)\n",
    "            torch.cuda.empty_cache()\n",
    "        return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Extracting features from training, validation and test data...')\n",
    "features_train = extract_features(feature_extractor, train_imgs_dataloader, pca)\n",
    "features_val = extract_features(feature_extractor, val_imgs_dataloader, pca)\n",
    "features_test = extract_features(feature_extractor, test_imgs_dataloader, pca)\n",
    "del model, pca, feature_extractor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VRAM, RAM and CACHE cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244797440\n",
      "799014912\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244797440\n",
      "799014912\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Module(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "model.to('cpu') # sposto sulla ram\n",
    "feature_extractor.to('cpu') # sposto sulla ram\n",
    "del model, feature_extractor, pca # elimino dalla ram\n",
    "torch.cuda.empty_cache() # elimino la chache vram\n",
    "gc.collect() # elimino la cache ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, feature_extractor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "2097152\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comulative Explained variance ratio:  0.5664719757071206\n",
      "Number of components:  300\n"
     ]
    }
   ],
   "source": [
    "pca_batch_size = 1000\n",
    "pca = IncrementalPCA(n_components=pca_component, batch_size=pca_batch_size).fit(features_train)\n",
    "print(\"Comulative Explained variance ratio: \", sum(pca.explained_variance_ratio_))\n",
    "print(\"Number of components: \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comulative Explained variance ratio:  0.5763521306798793\n",
      "Number of components:  300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=pca_component).fit(features_train)\n",
    "print(\"Comulative Explained variance ratio: \", sum(pca.explained_variance_ratio_))\n",
    "print(\"Number of components: \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algonauts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
