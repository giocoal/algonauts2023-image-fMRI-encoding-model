{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'jupyter_notebook' #@param ['colab', 'jupyter_notebook'] {allow-input: true}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision.models import AlexNet_Weights, VGG16_Weights, VGG16_BN_Weights\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from scipy.stats import pearsonr as corr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Platform Definition (jupyter or colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == 'jupyter_notebook':\n",
    "    # data_dir = '../../../Projects/Datasets/Biomedical/algonauts_2023_challenge_data'\n",
    "    # Data folder definition\n",
    "    data_dir = '../../Datasets/Biomedical/algonauts_2023_challenge_data'\n",
    "    # Used to save the prediction of saved model\n",
    "    parent_submission_dir = '../algonauts_2023_challenge_submission'\n",
    "    ncsnr_dir = '../../Datasets/Biomedical/algonauts_ncsnr'\n",
    "    images_trials = '../../Datasets/Biomedical/algonauts_train_images_trials'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select CPU or GPU and verify the selected device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n",
    "device = torch.device(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CUDA and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x000001939E531160>\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "print(torch.cuda.device(0))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version：\n",
      "1.13.0\n",
      "CUDA Version: \n",
      "11.6\n",
      "cuDNN version is :\n",
      "8302\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch version：\")\n",
    "print(torch.__version__)\n",
    "print(\"CUDA Version: \")\n",
    "print(torch.version.cuda)\n",
    "print(\"cuDNN version is :\")\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, choose which of the 8 subjects you will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some paths that we will need for loading and storing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj, parent_ncsnr_dir = ncsnr_dir, images_trials_parent_dir = images_trials):\n",
    "    # Define the dir where data is stored\n",
    "    # 1 became 01\n",
    "    self.subj = format(subj, '02') # '0numberofchars'\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "\n",
    "    # NCSNR\n",
    "    self.parent_ncsnr_dir = parent_ncsnr_dir\n",
    "    self.ncsnr_dir = os.path.join(self.parent_ncsnr_dir, 'subj'+self.subj)\n",
    "    \n",
    "    # SUBMISSION DIR\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "    # Train Images Trials \n",
    "    self.images_trials_parent_dir = images_trials_parent_dir\n",
    "    self.images_trials_dir = os.path.join(self.images_trials_parent_dir, 'subj'+self.subj)\n",
    "    \n",
    "args = argObj(data_dir, parent_submission_dir, subj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fMRI Pre-processed Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fMRI response for the selected subject\n",
    "- Response vectors are divided in left and right emisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n"
     ]
    }
   ],
   "source": [
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.0016644258\n",
      "Standard Deviation: 0.7087964\n",
      "Variance: 0.50239235\n",
      "Max: 6.3958163\n",
      "Min: -5.5488534\n",
      "Median: 0.0039879945\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean:\", np.mean(lh_fmri))\n",
    "print(\"Standard Deviation:\", np.std(lh_fmri))\n",
    "print(\"Variance:\", np.var(lh_fmri))\n",
    "print(\"Max:\", np.max(lh_fmri))\n",
    "print(\"Min:\", np.min(lh_fmri))\n",
    "print(\"Median:\", np.median(lh_fmri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8617882  -0.20318632 -0.62639767]\n",
      " [ 0.72242236 -0.2831422   0.92416   ]\n",
      " [-0.30844456 -0.9322294   1.7191527 ]]\n"
     ]
    }
   ],
   "source": [
    "print(lh_fmri[:3, :3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimulus Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images consist of natural scenes coming from the [COCO dataset][coco].\n",
    "\n",
    "The images are divided into a training and a test split (corresponding to the fMRI training and test data splits). The amount of training and test images varies between subjects.\n",
    "\n",
    "For more information on the images please see the [`README.txt`][readme] file accompanying the Challenge data.\n",
    "\n",
    "[coco]: https://cocodataset.org/#home\n",
    "[readme]: https://drive.google.com/file/d/16oLCaDmUBZuT6z_VGKO-qzwidYDE77Sg/view?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    }
   ],
   "source": [
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test images are stored in `.png` format. As an example, the first training image of subject 1 is named `train-0001_nsd-00013.png`.\n",
    "\n",
    "The first index (`'train-0001'`) orders the images so to match the stimulus images dimension of the fMRI training split data. This indexing starts from 1.\n",
    "\n",
    "The second index (`'nsd-00013'`) corresponds to the 73,000 NSD image IDs that you can use to map the image back to the [original `.hdf5` NSD image file][NSD_img_hdf5] (which contains all the 73,000 images used in the NSD experiment), and from there to the [COCO dataset][coco] images for metadata). The 73,000 NSD images IDs in the filename start from 0, so that you can directly use them for indexing the `.hdf5` NSD images in Python. Note that the images used in the NSD experiment (and here in the Algonauts 2023 Challenge) are cropped versions of the original COCO images. Therefore, if you wish to use the COCO image metadata you first need to adapt it to the cropped image coordinates. You can find code to perform this operation [here][coco_meta].\n",
    "\n",
    "[NSD_img_hdf5]: https://cvnlab.slite.page/p/NKalgWd__F/Experiments\n",
    "[coco]: https://cocodataset.org/#home\n",
    "[coco_meta]: https://github.com/styvesg/nsd_gnet8x/blob/main/data_preparation.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NSD ID Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training image file name: train-0001_nsd-00013.png\n",
      "73k NSD images ID: 00013\n"
     ]
    }
   ],
   "source": [
    "train_img_file = train_img_list[0]\n",
    "print('Training image file name: ' + train_img_file)\n",
    "print('73k NSD images ID: ' + train_img_file[-9:-4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate linearizing encoding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will build and evaluate [linearizing encoding models][encoding] using a pretrained [AlexNet][alexnet] architecture. You will train these models on the training partition and cross-validated them on the validation partition (an independent partition of the training data). The linearizing encoding algorithm involves the following steps:\n",
    "\n",
    "1. Split the data into training, validation and test partitions.\n",
    "\n",
    "2. Extract and downsample image features from AlexNet.\n",
    "\n",
    "3. Linearly map the AlexNet image feature to fMRI responses. \n",
    "\n",
    "4. Evaluate and visualize the encoding model's prediction accuracy (i.e., encoding accuracy) using the validation partition.\n",
    "\n",
    "You will use the test partition only in the final section of this tutorial, to prepare the Challenge submission files.\n",
    "\n",
    "[encoding]: https://www.sciencedirect.com/science/article/pii/S1053811910010657\n",
    "[alexnet]: https://arxiv.org/abs/1404.5997"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Create the training, validation and test partitions indices\n",
    "\n",
    "Here you will create the indices to randomly split the training data into a training (90% of the data) and validation (10% of the data) partition. You can enforce different random splits by editing the random seed variable (`rand_seed`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stimulus images: 8857\n",
      "\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "rand_seed = 5 #@param\n",
    "np.random.seed(rand_seed)\n",
    "train_percentage = 90\n",
    "\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * train_percentage))\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Create the training, validation and test image partitions DataLoaders\n",
    "\n",
    "We will use the `Dataset` and `DataLoader` classes from PyTorch to create our training, validation and test image partitions. You can read more about these type of classes and how to use them [here][data_tutorial_pytorch].\n",
    "\n",
    "Let's first define the preprocessing (transform) that will be applied to the images before feeding them to AlexNet. We will use a [standard preprocessing pipeline][preprocessing] as used in the computer vision literature.\n",
    "\n",
    "[data_tutorial_pytorch]: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "[preprocessing]: https://pytorch.org/hub/pytorch_vision_alexnet/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), # resize the images to 224x224 pixels\n",
    "    transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    # normalize the images color channels\n",
    "    # mean: [0.485, 0.456, 0.406] for the three channels\n",
    "    # std: [0.229, 0.224, 0.225] for the three channels\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset class**: that will load, preprocess, and return an image at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            img = self.transform(img).to(device)\n",
    "        return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a **DataLoader class** that allow us to iterate over batches of images. We use batches because loading and processing all images at once uses too much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images:  9841\n",
      "Remainder:  301\n",
      "Batch size:  318\n"
     ]
    }
   ],
   "source": [
    "# Create a list of 8000 zeros\n",
    "batch_size_min = 300\n",
    "batch_size_max = 350\n",
    "\n",
    "for batch_size in range(batch_size_min, batch_size_max + 1):\n",
    "    if len(train_img_list) % batch_size >= batch_size_min:\n",
    "        print(\"Train images: \", len(train_img_list))\n",
    "        print(\"Remainder: \", len(train_img_list) % batch_size)\n",
    "        print(\"Batch size: \", batch_size)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 300 # 300 #@param (310 for 3 and 6, 300 for others)\n",
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_train, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_val, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test, transform), \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Split the fMRI data into training and validation partitions\n",
    "\n",
    "Here we will use the previously defined indices to split the training fMRI data into a training and validation partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the original fMRI training split to free up RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lh_fmri, rh_fmri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extract and downsample image features from AlexNet\n",
    "\n",
    "In this step we will extract image features from a pretrained AlexNet, and downsample them to 100 PCA components to speed up computations in the the next encoding step (mapping of AlexNet feature onto fMRI features)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Load the pretrained AlexNet\n",
    "\n",
    "We will load the pretrained AlexNet from the [PyTorch Hub][pytorch_hub] and set it to evaluation mode since we will not be training this network.\n",
    "\n",
    "[pytorch_hub]: https://pytorch.org/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\giorg/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet is composed of 5 convolutional layers (contained in a submodule called 'features') and 3 linear layers (contained in a submodule named 'classifier').\n",
    "\n",
    "First let's print the AlexNet layer names of the PyTorch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n"
     ]
    }
   ],
   "source": [
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print(train_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Extract and downsample the AlexNet features\n",
    "\n",
    "We will now extract the image features of an AlexNet layer of your choice, and reduce their size using PCA.\n",
    "\n",
    "To extract the features, we will use the PyTorch function [`create_feature_extractor`][extract_feat].\n",
    "\n",
    "1. First, choose the AlexNet layer you wish to use.\n",
    "   -  The base choice is `features.2` → which correspond to the **MaxPool Layer of the FIrst Convolutional Block**\n",
    "   - `(2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)`\n",
    "\n",
    "[extract_feat]: https://pytorch.org/vision/stable/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer = \"features.12\" #@param [\"features.2\", \"features.5\", \"features.7\", \"features.9\", \"features.12\", \"classifier.2\", \"classifier.5\", \"classifier.6\"] {allow-input: true}\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the feature vectors we want to extract is very large, so we will use a dimensionality reduction technique (in this case PCA) to overcome this problem. We will fit the PCA on the training images features, and use it to downsample the training, validation and test images features.\n",
    "\n",
    "In an *ideal scenario with no RAM limitations, we would compute the PCA and transform all images of the training set simultaneously*. Since we do have this restriction, we will do a **batch-wise partial computation of the PCA** (see [`IncrementalPCA`][pca] for more info).\n",
    "\n",
    "The following function extracts the AlexNet features from each image batch, and partially computes the PCA.\n",
    "\n",
    "[pca]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=100, batch_size=batch_size)\n",
    "\n",
    "    # Fit PCA to batch\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the PCA on the training partition images features. This step can take longer or shorter depending on the AlexNet layer you chose to use for the encoding, and on your computing resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [02:29<00:00,  5.35s/it]\n"
     ]
    }
   ],
   "source": [
    "pca = fit_pca(feature_extractor, train_imgs_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we fitted our PCA model, we will use it do reduce the dimensionality of the training, validation and test images features.\n",
    "\n",
    "Unfortunately, due to memory limits we will have to extract the AlexNet image features from each batch again to downsample them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(feature_extractor, dataloader, pca):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        ft = pca.transform(ft.cpu().detach().numpy())\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:46<00:00,  3.82s/it]\n",
      "100%|██████████| 4/4 [00:19<00:00,  4.78s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "features_train = extract_features(feature_extractor, train_imgs_dataloader, pca)\n",
    "features_val = extract_features(feature_extractor, val_imgs_dataloader, pca)\n",
    "features_test = extract_features(feature_extractor, test_imgs_dataloader, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training images features:\n",
      "(8857, 100)\n",
      "(Training stimulus images × PCA features)\n",
      "\n",
      "Validation images features:\n",
      "(984, 100)\n",
      "(Validation stimulus images × PCA features)\n",
      "\n",
      "Test images features:\n",
      "(159, 100)\n",
      "(Test stimulus images × PCA features)\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining images features:')\n",
    "print(features_train.shape)\n",
    "print('(Training stimulus images × PCA features)')\n",
    "\n",
    "print('\\nValidation images features:')\n",
    "print(features_val.shape)\n",
    "print('(Validation stimulus images × PCA features)')\n",
    "\n",
    "print('\\nTest images features:')\n",
    "print(features_test.shape)\n",
    "print('(Test stimulus images × PCA features)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free up RAM by deleting the AlexNet model and PCA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, pca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained efficient-netb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import EfficientNet_B2_Weights\n",
    "from torchvision import models\n",
    "model = models.efficientnet_b2(weights = EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'features.0', 'features.1.0.block.0', 'features.1.0.block.1', 'features.1.0.block.2', 'features.1.1.block.0', 'features.1.1.block.1', 'features.1.1.block.2', 'features.1.1.stochastic_depth', 'features.1.1.add', 'features.2.0.block.0', 'features.2.0.block.1', 'features.2.0.block.2', 'features.2.0.block.3', 'features.2.1.block.0', 'features.2.1.block.1', 'features.2.1.block.2', 'features.2.1.block.3', 'features.2.1.stochastic_depth', 'features.2.1.add', 'features.2.2.block.0', 'features.2.2.block.1', 'features.2.2.block.2', 'features.2.2.block.3', 'features.2.2.stochastic_depth', 'features.2.2.add', 'features.3.0.block.0', 'features.3.0.block.1', 'features.3.0.block.2', 'features.3.0.block.3', 'features.3.1.block.0', 'features.3.1.block.1', 'features.3.1.block.2', 'features.3.1.block.3', 'features.3.1.stochastic_depth', 'features.3.1.add', 'features.3.2.block.0', 'features.3.2.block.1', 'features.3.2.block.2', 'features.3.2.block.3', 'features.3.2.stochastic_depth', 'features.3.2.add', 'features.4.0.block.0', 'features.4.0.block.1', 'features.4.0.block.2', 'features.4.0.block.3', 'features.4.1.block.0', 'features.4.1.block.1', 'features.4.1.block.2', 'features.4.1.block.3', 'features.4.1.stochastic_depth', 'features.4.1.add', 'features.4.2.block.0', 'features.4.2.block.1', 'features.4.2.block.2', 'features.4.2.block.3', 'features.4.2.stochastic_depth', 'features.4.2.add', 'features.4.3.block.0', 'features.4.3.block.1', 'features.4.3.block.2', 'features.4.3.block.3', 'features.4.3.stochastic_depth', 'features.4.3.add', 'features.5.0.block.0', 'features.5.0.block.1', 'features.5.0.block.2', 'features.5.0.block.3', 'features.5.1.block.0', 'features.5.1.block.1', 'features.5.1.block.2', 'features.5.1.block.3', 'features.5.1.stochastic_depth', 'features.5.1.add', 'features.5.2.block.0', 'features.5.2.block.1', 'features.5.2.block.2', 'features.5.2.block.3', 'features.5.2.stochastic_depth', 'features.5.2.add', 'features.5.3.block.0', 'features.5.3.block.1', 'features.5.3.block.2', 'features.5.3.block.3', 'features.5.3.stochastic_depth', 'features.5.3.add', 'features.6.0.block.0', 'features.6.0.block.1', 'features.6.0.block.2', 'features.6.0.block.3', 'features.6.1.block.0', 'features.6.1.block.1', 'features.6.1.block.2', 'features.6.1.block.3', 'features.6.1.stochastic_depth', 'features.6.1.add', 'features.6.2.block.0', 'features.6.2.block.1', 'features.6.2.block.2', 'features.6.2.block.3', 'features.6.2.stochastic_depth', 'features.6.2.add', 'features.6.3.block.0', 'features.6.3.block.1', 'features.6.3.block.2', 'features.6.3.block.3', 'features.6.3.stochastic_depth', 'features.6.3.add', 'features.6.4.block.0', 'features.6.4.block.1', 'features.6.4.block.2', 'features.6.4.block.3', 'features.6.4.stochastic_depth', 'features.6.4.add', 'features.7.0.block.0', 'features.7.0.block.1', 'features.7.0.block.2', 'features.7.0.block.3', 'features.7.1.block.0', 'features.7.1.block.1', 'features.7.1.block.2', 'features.7.1.block.3', 'features.7.1.stochastic_depth', 'features.7.1.add', 'features.8', 'avgpool', 'flatten', 'classifier.0', 'classifier.1']\n"
     ]
    }
   ],
   "source": [
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print(train_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\giorg/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', \n",
    "                        'vgg16_bn', \n",
    "                        weights=VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'features.31', 'features.32', 'features.33', 'features.34', 'features.35', 'features.36', 'features.37', 'features.38', 'features.39', 'features.40', 'features.41', 'features.42', 'features.43', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n"
     ]
    }
   ],
   "source": [
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print(train_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', \n",
    "                        'vgg16', \n",
    "                        weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\giorg/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\giorg/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb490505c86142018280ad9ec2d09f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import VGG19_Weights\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', \n",
    "                        'vgg19', \n",
    "                        weights=VGG19_Weights.IMAGENET1K_V1)\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PCA specifiyng explained variance percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=0.30, batch_size=batch_size)\n",
    "\n",
    "    # Fit PCA to batch\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = fit_pca(feature_extractor, train_imgs_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comulative Explained variance ratio:  0.39269976662655187\n",
      "Number of components:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"Comulative Explained variance ratio: \", sum(pca.explained_variance_ratio_))\n",
    "print(\"Number of components: \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39269976662655187"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly map the AlexNet image features to fMRI responses + compute pearson correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regressions on the training data\n",
    "reg_lh = LinearRegression().fit(features_train, lh_fmri_train)\n",
    "reg_rh = LinearRegression().fit(features_train, rh_fmri_train)\n",
    "# Use fitted linear regressions to predict the validation and test fMRI data\n",
    "lh_fmri_val_pred = reg_lh.predict(features_val)\n",
    "lh_fmri_test_pred = reg_lh.predict(features_test)\n",
    "rh_fmri_val_pred = reg_rh.predict(features_val)\n",
    "rh_fmri_test_pred = reg_rh.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(908, 19004)\n",
      "Mean: 0.00526684624765987\n",
      "Standard deviation: 0.2042430403708923\n",
      "Minimum: -1.573190950137589\n",
      "Maximum: 1.8854053635788137\n",
      "Median: 0.0017901554197446686\n",
      "25th percentile: -0.10286238085740623\n",
      "75th percentile: 0.11017703774850804\n"
     ]
    }
   ],
   "source": [
    "# Explore the numpy arrays\n",
    "print(lh_fmri_val_pred.shape)\n",
    "\n",
    "# Print summary statistics on lh_fmri_val_pred\n",
    "print('Mean: ' + format(np.mean(lh_fmri_val_pred)))\n",
    "print('Standard deviation: ' + format(np.std(lh_fmri_val_pred)))\n",
    "print('Minimum: ' + format(np.min(lh_fmri_val_pred)))\n",
    "print('Maximum: ' + format(np.max(lh_fmri_val_pred)))\n",
    "print('Median: ' + format(np.median(lh_fmri_val_pred)))\n",
    "print('25th percentile: ' + format(np.percentile(lh_fmri_val_pred, 25)))\n",
    "print('75th percentile: ' + format(np.percentile(lh_fmri_val_pred, 75)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search + Cross Validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.0001, 0.0002, 0.001, 0.01, 0.1, 1, 10, 100, 1e4, 2e4, 5e4, 1e5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perason_numpy(pred, target):\n",
    "\n",
    "    corrcoef = list()\n",
    "    for pred, target in zip(pred.T, target.T):\n",
    "\n",
    "        s, _ = pearsonr(x=pred, y=target)\n",
    "        corrcoef.append(s)\n",
    "\n",
    "    return np.array(corrcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Param: {'alpha': 100000.0}\n",
      "Best Score: 0.4003252882147619\n"
     ]
    }
   ],
   "source": [
    "grid_search_l = GridSearchCV(Ridge(), param_grid=param_grid, scoring=make_scorer(\n",
    "    lambda x, y: np.median(compute_perason_numpy(x, y))), cv=5, n_jobs=5, verbose=1)\n",
    "grid_search_l.fit(X=features_train, y=lh_fmri_train)\n",
    "print(\"Best Param: {}\".format(grid_search_l.best_params_))\n",
    "print(\"Best Score: {}\".format(grid_search_l.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param: {'alpha': 100000.0}\n",
      "Best Score: 0.4003252882147619\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Param: {}\".format(grid_search_l.best_params_))\n",
    "print(\"Best Score: {}\".format(grid_search_l.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_l.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Param: {'alpha': 100000.0}\n",
      "Best Score: 0.40087493401053653\n"
     ]
    }
   ],
   "source": [
    "grid_search_r = GridSearchCV(Ridge(), param_grid=param_grid, scoring=make_scorer(\n",
    "    lambda x, y: np.median(compute_perason_numpy(x, y))), cv=5, n_jobs=5, verbose=1)\n",
    "grid_search_r.fit(X=features_train, y=rh_fmri_train)\n",
    "print(\"Best Param: {}\".format(grid_search_r.best_params_))\n",
    "print(\"Best Score: {}\".format(grid_search_r.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_r.cv_results_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   15.6s remaining:   23.5s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fit_time>:\t6.887,\t7.227,\t6.981,\t7.071,\t7.119\n",
      "<score_time>:\t8.284,\t7.967,\t8.241,\t8.177,\t8.139\n",
      "<test_score>:\t0.394,\t0.403,\t0.399,\t0.399,\t0.396\n"
     ]
    }
   ],
   "source": [
    "model_l = cross_validate(Ridge(alpha=1e4), X=features_train, y=lh_fmri_train, cv=5, n_jobs=5,\n",
    "                         scoring=make_scorer(lambda x, y: np.median(compute_perason_numpy(x, y))), verbose=1)\n",
    "\n",
    "for k, v in model_l.items():\n",
    "    print(\"<{}>:\\t{}\".format(k, \",\\t\".join([\"{:.3f}\".format(x) for x in v])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   15.5s remaining:   23.3s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fit_time>:\t7.741,\t7.713,\t7.818,\t7.778,\t7.813\n",
      "<score_time>:\t7.520,\t7.358,\t7.281,\t7.480,\t7.363\n",
      "<test_score>:\t0.394,\t0.406,\t0.400,\t0.398,\t0.396\n"
     ]
    }
   ],
   "source": [
    "model_r = cross_validate(Ridge(alpha=1e4), X=features_train, y=rh_fmri_train, cv=5, n_jobs=5,\n",
    "                         scoring=make_scorer(lambda x, y: np.median(compute_perason_numpy(x, y))), verbose=1)\n",
    "\n",
    "for k, v in model_r.items():\n",
    "    print(\"<{}>:\\t{}\".format(k, \",\\t\".join([\"{:.3f}\".format(x) for x in v])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the encoding accuracy through a Pearson's correlation\n",
    "\n",
    "Here you will compute your encoding model's prediction accuracy (or encoding accuracy) through a Pearson correlation between the predicted and ground truth fMRI validation partition data. The correlation scores indicate how similar the predicted fMRI data is to the ground truth data, namely how accurate your encoding model is in predicting (encoding) fMRI responses to images.\n",
    "\n",
    "Specifically, you will correlate the activity of each predicted fMRI vertex with the activity of the corresponding ground truth fMRI vertex, across the validation partition stimulus images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19004/19004 [00:01<00:00, 10895.38it/s]\n",
      "100%|██████████| 20544/20544 [00:01<00:00, 11119.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Empty correlation array of shape: (LH vertices)\n",
    "lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(lh_fmri_val_pred.shape[1])):\n",
    "    lh_correlation[v] = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0] # 0 per selezionare valore e non p-value\n",
    "\n",
    "# Empty correlation array of shape: (RH vertices)\n",
    "rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
    "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(rh_fmri_val_pred.shape[1])):\n",
    "    rh_correlation[v] = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v18995</th>\n",
       "      <th>v18996</th>\n",
       "      <th>v18997</th>\n",
       "      <th>v18998</th>\n",
       "      <th>v18999</th>\n",
       "      <th>v19000</th>\n",
       "      <th>v19001</th>\n",
       "      <th>v19002</th>\n",
       "      <th>v19003</th>\n",
       "      <th>v19004</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>img1</th>\n",
       "      <td>-1.362684</td>\n",
       "      <td>0.105403</td>\n",
       "      <td>-0.634885</td>\n",
       "      <td>1.116996</td>\n",
       "      <td>-1.146858</td>\n",
       "      <td>-0.142268</td>\n",
       "      <td>-0.450975</td>\n",
       "      <td>-0.201355</td>\n",
       "      <td>-1.020084</td>\n",
       "      <td>-1.626789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456325</td>\n",
       "      <td>0.285525</td>\n",
       "      <td>-0.036011</td>\n",
       "      <td>-0.006968</td>\n",
       "      <td>-0.372848</td>\n",
       "      <td>-0.372848</td>\n",
       "      <td>-0.203140</td>\n",
       "      <td>-0.107464</td>\n",
       "      <td>-0.016841</td>\n",
       "      <td>-0.391263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img2</th>\n",
       "      <td>-0.755504</td>\n",
       "      <td>-0.152065</td>\n",
       "      <td>-0.998677</td>\n",
       "      <td>-0.694289</td>\n",
       "      <td>-0.747618</td>\n",
       "      <td>-0.024071</td>\n",
       "      <td>-0.754630</td>\n",
       "      <td>-0.922632</td>\n",
       "      <td>-1.615000</td>\n",
       "      <td>0.043055</td>\n",
       "      <td>...</td>\n",
       "      <td>1.297180</td>\n",
       "      <td>1.459296</td>\n",
       "      <td>1.025424</td>\n",
       "      <td>1.005834</td>\n",
       "      <td>0.723775</td>\n",
       "      <td>0.723775</td>\n",
       "      <td>-0.095860</td>\n",
       "      <td>0.136672</td>\n",
       "      <td>0.148448</td>\n",
       "      <td>0.380446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img3</th>\n",
       "      <td>0.864619</td>\n",
       "      <td>0.136462</td>\n",
       "      <td>-0.621923</td>\n",
       "      <td>0.023420</td>\n",
       "      <td>-1.307619</td>\n",
       "      <td>-0.460672</td>\n",
       "      <td>1.176813</td>\n",
       "      <td>-1.424946</td>\n",
       "      <td>0.715326</td>\n",
       "      <td>-0.628299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>-0.010651</td>\n",
       "      <td>0.345488</td>\n",
       "      <td>0.391329</td>\n",
       "      <td>0.307540</td>\n",
       "      <td>0.307540</td>\n",
       "      <td>-0.494696</td>\n",
       "      <td>-0.472332</td>\n",
       "      <td>-0.482341</td>\n",
       "      <td>-0.727570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img4</th>\n",
       "      <td>1.017041</td>\n",
       "      <td>-1.972292</td>\n",
       "      <td>0.777953</td>\n",
       "      <td>0.681789</td>\n",
       "      <td>0.483763</td>\n",
       "      <td>0.744049</td>\n",
       "      <td>0.799378</td>\n",
       "      <td>1.427946</td>\n",
       "      <td>0.780410</td>\n",
       "      <td>0.517839</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145668</td>\n",
       "      <td>-0.158004</td>\n",
       "      <td>-0.662477</td>\n",
       "      <td>-0.014418</td>\n",
       "      <td>-0.469669</td>\n",
       "      <td>-0.469669</td>\n",
       "      <td>-0.132624</td>\n",
       "      <td>-0.466943</td>\n",
       "      <td>-0.257576</td>\n",
       "      <td>0.203096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img5</th>\n",
       "      <td>0.468452</td>\n",
       "      <td>0.146705</td>\n",
       "      <td>-0.446328</td>\n",
       "      <td>1.134242</td>\n",
       "      <td>0.489677</td>\n",
       "      <td>0.092135</td>\n",
       "      <td>0.812579</td>\n",
       "      <td>-0.042108</td>\n",
       "      <td>0.319703</td>\n",
       "      <td>0.822444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082524</td>\n",
       "      <td>-0.621402</td>\n",
       "      <td>0.014672</td>\n",
       "      <td>0.275363</td>\n",
       "      <td>-0.293559</td>\n",
       "      <td>-0.293559</td>\n",
       "      <td>-0.402352</td>\n",
       "      <td>-0.426450</td>\n",
       "      <td>-0.489094</td>\n",
       "      <td>-0.634863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img980</th>\n",
       "      <td>-0.232234</td>\n",
       "      <td>-0.299230</td>\n",
       "      <td>0.065471</td>\n",
       "      <td>0.992445</td>\n",
       "      <td>-0.598850</td>\n",
       "      <td>0.041030</td>\n",
       "      <td>0.742306</td>\n",
       "      <td>-0.892351</td>\n",
       "      <td>-0.500703</td>\n",
       "      <td>-0.486626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005898</td>\n",
       "      <td>0.332630</td>\n",
       "      <td>0.334730</td>\n",
       "      <td>-0.358881</td>\n",
       "      <td>-0.936860</td>\n",
       "      <td>-0.936860</td>\n",
       "      <td>-0.240730</td>\n",
       "      <td>-0.313589</td>\n",
       "      <td>-0.234719</td>\n",
       "      <td>0.337383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img981</th>\n",
       "      <td>0.075665</td>\n",
       "      <td>-0.148535</td>\n",
       "      <td>-0.708590</td>\n",
       "      <td>-0.623013</td>\n",
       "      <td>0.364155</td>\n",
       "      <td>-0.745957</td>\n",
       "      <td>1.450083</td>\n",
       "      <td>-1.272296</td>\n",
       "      <td>-0.610939</td>\n",
       "      <td>1.130801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205289</td>\n",
       "      <td>-0.162905</td>\n",
       "      <td>-0.367591</td>\n",
       "      <td>-0.564725</td>\n",
       "      <td>-0.362926</td>\n",
       "      <td>-0.362926</td>\n",
       "      <td>-0.209540</td>\n",
       "      <td>-0.804745</td>\n",
       "      <td>-0.727909</td>\n",
       "      <td>-0.005451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img982</th>\n",
       "      <td>-0.686496</td>\n",
       "      <td>-0.551945</td>\n",
       "      <td>-2.309315</td>\n",
       "      <td>-0.965421</td>\n",
       "      <td>-0.944869</td>\n",
       "      <td>-1.542230</td>\n",
       "      <td>-0.031113</td>\n",
       "      <td>-1.798432</td>\n",
       "      <td>-0.525722</td>\n",
       "      <td>-0.456545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980871</td>\n",
       "      <td>-0.678518</td>\n",
       "      <td>-0.724203</td>\n",
       "      <td>-0.629889</td>\n",
       "      <td>-0.163823</td>\n",
       "      <td>-0.163823</td>\n",
       "      <td>-0.931302</td>\n",
       "      <td>-0.667792</td>\n",
       "      <td>-0.594068</td>\n",
       "      <td>-0.201914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img983</th>\n",
       "      <td>-0.119940</td>\n",
       "      <td>0.083208</td>\n",
       "      <td>0.310171</td>\n",
       "      <td>1.142716</td>\n",
       "      <td>0.038672</td>\n",
       "      <td>-0.325022</td>\n",
       "      <td>0.788824</td>\n",
       "      <td>0.684125</td>\n",
       "      <td>-0.362595</td>\n",
       "      <td>0.128404</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.395679</td>\n",
       "      <td>-1.675182</td>\n",
       "      <td>-1.117538</td>\n",
       "      <td>-0.559994</td>\n",
       "      <td>0.343039</td>\n",
       "      <td>0.343039</td>\n",
       "      <td>-0.209873</td>\n",
       "      <td>-0.123184</td>\n",
       "      <td>-0.179543</td>\n",
       "      <td>-0.510203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img984</th>\n",
       "      <td>-0.064329</td>\n",
       "      <td>0.726094</td>\n",
       "      <td>-0.683798</td>\n",
       "      <td>-0.210676</td>\n",
       "      <td>-2.024465</td>\n",
       "      <td>-0.381887</td>\n",
       "      <td>1.282394</td>\n",
       "      <td>0.585238</td>\n",
       "      <td>-1.422317</td>\n",
       "      <td>-1.719944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195366</td>\n",
       "      <td>-0.039021</td>\n",
       "      <td>0.084541</td>\n",
       "      <td>0.456954</td>\n",
       "      <td>1.233447</td>\n",
       "      <td>1.233447</td>\n",
       "      <td>0.306251</td>\n",
       "      <td>0.069216</td>\n",
       "      <td>0.154815</td>\n",
       "      <td>-0.585645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows × 19004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              v1        v2        v3        v4        v5        v6        v7  \\\n",
       "img1   -1.362684  0.105403 -0.634885  1.116996 -1.146858 -0.142268 -0.450975   \n",
       "img2   -0.755504 -0.152065 -0.998677 -0.694289 -0.747618 -0.024071 -0.754630   \n",
       "img3    0.864619  0.136462 -0.621923  0.023420 -1.307619 -0.460672  1.176813   \n",
       "img4    1.017041 -1.972292  0.777953  0.681789  0.483763  0.744049  0.799378   \n",
       "img5    0.468452  0.146705 -0.446328  1.134242  0.489677  0.092135  0.812579   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "img980 -0.232234 -0.299230  0.065471  0.992445 -0.598850  0.041030  0.742306   \n",
       "img981  0.075665 -0.148535 -0.708590 -0.623013  0.364155 -0.745957  1.450083   \n",
       "img982 -0.686496 -0.551945 -2.309315 -0.965421 -0.944869 -1.542230 -0.031113   \n",
       "img983 -0.119940  0.083208  0.310171  1.142716  0.038672 -0.325022  0.788824   \n",
       "img984 -0.064329  0.726094 -0.683798 -0.210676 -2.024465 -0.381887  1.282394   \n",
       "\n",
       "              v8        v9       v10  ...    v18995    v18996    v18997  \\\n",
       "img1   -0.201355 -1.020084 -1.626789  ...  0.456325  0.285525 -0.036011   \n",
       "img2   -0.922632 -1.615000  0.043055  ...  1.297180  1.459296  1.025424   \n",
       "img3   -1.424946  0.715326 -0.628299  ... -0.001527 -0.010651  0.345488   \n",
       "img4    1.427946  0.780410  0.517839  ... -0.145668 -0.158004 -0.662477   \n",
       "img5   -0.042108  0.319703  0.822444  ... -0.082524 -0.621402  0.014672   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "img980 -0.892351 -0.500703 -0.486626  ... -0.005898  0.332630  0.334730   \n",
       "img981 -1.272296 -0.610939  1.130801  ... -0.205289 -0.162905 -0.367591   \n",
       "img982 -1.798432 -0.525722 -0.456545  ... -0.980871 -0.678518 -0.724203   \n",
       "img983  0.684125 -0.362595  0.128404  ... -1.395679 -1.675182 -1.117538   \n",
       "img984  0.585238 -1.422317 -1.719944  ...  0.195366 -0.039021  0.084541   \n",
       "\n",
       "          v18998    v18999    v19000    v19001    v19002    v19003    v19004  \n",
       "img1   -0.006968 -0.372848 -0.372848 -0.203140 -0.107464 -0.016841 -0.391263  \n",
       "img2    1.005834  0.723775  0.723775 -0.095860  0.136672  0.148448  0.380446  \n",
       "img3    0.391329  0.307540  0.307540 -0.494696 -0.472332 -0.482341 -0.727570  \n",
       "img4   -0.014418 -0.469669 -0.469669 -0.132624 -0.466943 -0.257576  0.203096  \n",
       "img5    0.275363 -0.293559 -0.293559 -0.402352 -0.426450 -0.489094 -0.634863  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "img980 -0.358881 -0.936860 -0.936860 -0.240730 -0.313589 -0.234719  0.337383  \n",
       "img981 -0.564725 -0.362926 -0.362926 -0.209540 -0.804745 -0.727909 -0.005451  \n",
       "img982 -0.629889 -0.163823 -0.163823 -0.931302 -0.667792 -0.594068 -0.201914  \n",
       "img983 -0.559994  0.343039  0.343039 -0.209873 -0.123184 -0.179543 -0.510203  \n",
       "img984  0.456954  1.233447  1.233447  0.306251  0.069216  0.154815 -0.585645  \n",
       "\n",
       "[984 rows x 19004 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "row_labels = ['img'+str(i+1) for i in range(lh_fmri_val.shape[0])]\n",
    "col_labels = ['v'+str(i+1) for i in range(lh_fmri_val.shape[1])]\n",
    "\n",
    "# Creazione del DataFrame\n",
    "fmri_val_df = pd.DataFrame(lh_fmri_val, index=row_labels, columns=col_labels)\n",
    "fmri_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v18995</th>\n",
       "      <th>v18996</th>\n",
       "      <th>v18997</th>\n",
       "      <th>v18998</th>\n",
       "      <th>v18999</th>\n",
       "      <th>v19000</th>\n",
       "      <th>v19001</th>\n",
       "      <th>v19002</th>\n",
       "      <th>v19003</th>\n",
       "      <th>v19004</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>img1</th>\n",
       "      <td>-0.290258</td>\n",
       "      <td>-0.260766</td>\n",
       "      <td>0.398556</td>\n",
       "      <td>0.551459</td>\n",
       "      <td>0.203292</td>\n",
       "      <td>0.350700</td>\n",
       "      <td>0.090883</td>\n",
       "      <td>0.560466</td>\n",
       "      <td>0.076039</td>\n",
       "      <td>-0.470655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197475</td>\n",
       "      <td>-0.260196</td>\n",
       "      <td>-0.135420</td>\n",
       "      <td>-0.090778</td>\n",
       "      <td>-0.017567</td>\n",
       "      <td>-0.017567</td>\n",
       "      <td>-0.084397</td>\n",
       "      <td>-0.254089</td>\n",
       "      <td>-0.320621</td>\n",
       "      <td>-0.044061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img2</th>\n",
       "      <td>-0.231810</td>\n",
       "      <td>-0.086052</td>\n",
       "      <td>-0.523469</td>\n",
       "      <td>-0.638392</td>\n",
       "      <td>-0.316659</td>\n",
       "      <td>0.054627</td>\n",
       "      <td>-0.208353</td>\n",
       "      <td>-0.894260</td>\n",
       "      <td>-0.325251</td>\n",
       "      <td>-0.335083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265778</td>\n",
       "      <td>0.319153</td>\n",
       "      <td>0.250359</td>\n",
       "      <td>0.200973</td>\n",
       "      <td>0.152748</td>\n",
       "      <td>0.152748</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>0.038726</td>\n",
       "      <td>0.053515</td>\n",
       "      <td>0.055127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img3</th>\n",
       "      <td>0.167813</td>\n",
       "      <td>0.094185</td>\n",
       "      <td>0.182432</td>\n",
       "      <td>-0.162009</td>\n",
       "      <td>-0.086838</td>\n",
       "      <td>0.256734</td>\n",
       "      <td>-0.065228</td>\n",
       "      <td>-0.337406</td>\n",
       "      <td>-0.042966</td>\n",
       "      <td>-0.421225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337032</td>\n",
       "      <td>0.353427</td>\n",
       "      <td>0.318367</td>\n",
       "      <td>0.299446</td>\n",
       "      <td>0.242927</td>\n",
       "      <td>0.242927</td>\n",
       "      <td>0.316172</td>\n",
       "      <td>0.378080</td>\n",
       "      <td>0.363827</td>\n",
       "      <td>0.152742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img4</th>\n",
       "      <td>0.179203</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>-0.010540</td>\n",
       "      <td>0.228525</td>\n",
       "      <td>0.077062</td>\n",
       "      <td>0.064330</td>\n",
       "      <td>0.343766</td>\n",
       "      <td>0.627719</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>-0.000853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.089175</td>\n",
       "      <td>0.078179</td>\n",
       "      <td>0.075996</td>\n",
       "      <td>0.071723</td>\n",
       "      <td>0.071723</td>\n",
       "      <td>-0.008810</td>\n",
       "      <td>0.030115</td>\n",
       "      <td>0.040529</td>\n",
       "      <td>0.010019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img5</th>\n",
       "      <td>0.304406</td>\n",
       "      <td>0.443552</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.065611</td>\n",
       "      <td>-0.039035</td>\n",
       "      <td>0.138633</td>\n",
       "      <td>0.044665</td>\n",
       "      <td>-0.205013</td>\n",
       "      <td>-0.339522</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382797</td>\n",
       "      <td>0.463299</td>\n",
       "      <td>0.298858</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>0.101586</td>\n",
       "      <td>0.101586</td>\n",
       "      <td>0.114196</td>\n",
       "      <td>0.156807</td>\n",
       "      <td>0.192245</td>\n",
       "      <td>-0.022523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img980</th>\n",
       "      <td>-0.049621</td>\n",
       "      <td>-0.194991</td>\n",
       "      <td>-0.377691</td>\n",
       "      <td>-0.117684</td>\n",
       "      <td>-0.192404</td>\n",
       "      <td>-0.226072</td>\n",
       "      <td>-0.064908</td>\n",
       "      <td>-0.176825</td>\n",
       "      <td>-0.045374</td>\n",
       "      <td>-0.143872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037360</td>\n",
       "      <td>-0.060519</td>\n",
       "      <td>-0.017378</td>\n",
       "      <td>-0.001417</td>\n",
       "      <td>0.031624</td>\n",
       "      <td>0.031624</td>\n",
       "      <td>-0.009529</td>\n",
       "      <td>0.005759</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.072564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img981</th>\n",
       "      <td>-0.051303</td>\n",
       "      <td>-0.079236</td>\n",
       "      <td>-0.621526</td>\n",
       "      <td>-0.791669</td>\n",
       "      <td>-0.165708</td>\n",
       "      <td>-0.275625</td>\n",
       "      <td>-0.062718</td>\n",
       "      <td>-0.615584</td>\n",
       "      <td>0.181013</td>\n",
       "      <td>0.181578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062214</td>\n",
       "      <td>0.054557</td>\n",
       "      <td>0.067791</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>-0.030190</td>\n",
       "      <td>-0.030190</td>\n",
       "      <td>-0.025124</td>\n",
       "      <td>-0.154573</td>\n",
       "      <td>-0.176024</td>\n",
       "      <td>-0.111984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img982</th>\n",
       "      <td>0.026921</td>\n",
       "      <td>-0.110675</td>\n",
       "      <td>-1.328202</td>\n",
       "      <td>-0.824288</td>\n",
       "      <td>-0.489548</td>\n",
       "      <td>-0.511362</td>\n",
       "      <td>0.156399</td>\n",
       "      <td>-1.129562</td>\n",
       "      <td>0.256481</td>\n",
       "      <td>0.513830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255496</td>\n",
       "      <td>-0.145004</td>\n",
       "      <td>-0.229646</td>\n",
       "      <td>-0.270375</td>\n",
       "      <td>-0.281994</td>\n",
       "      <td>-0.281994</td>\n",
       "      <td>-0.473573</td>\n",
       "      <td>-0.484659</td>\n",
       "      <td>-0.413436</td>\n",
       "      <td>-0.319208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img983</th>\n",
       "      <td>0.154602</td>\n",
       "      <td>0.323428</td>\n",
       "      <td>0.574714</td>\n",
       "      <td>0.690337</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>-0.063628</td>\n",
       "      <td>0.121437</td>\n",
       "      <td>0.386864</td>\n",
       "      <td>-0.322702</td>\n",
       "      <td>-0.039456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020442</td>\n",
       "      <td>0.034058</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>-0.028001</td>\n",
       "      <td>-0.007571</td>\n",
       "      <td>-0.007571</td>\n",
       "      <td>0.134924</td>\n",
       "      <td>0.232575</td>\n",
       "      <td>0.249692</td>\n",
       "      <td>-0.005120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img984</th>\n",
       "      <td>-0.056947</td>\n",
       "      <td>0.232762</td>\n",
       "      <td>-0.199307</td>\n",
       "      <td>0.151706</td>\n",
       "      <td>-0.037999</td>\n",
       "      <td>-0.041623</td>\n",
       "      <td>-0.241919</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>-0.040044</td>\n",
       "      <td>-0.435984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113146</td>\n",
       "      <td>0.108532</td>\n",
       "      <td>0.146041</td>\n",
       "      <td>0.188329</td>\n",
       "      <td>0.158098</td>\n",
       "      <td>0.158098</td>\n",
       "      <td>0.228462</td>\n",
       "      <td>0.271481</td>\n",
       "      <td>0.256635</td>\n",
       "      <td>0.177602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows × 19004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              v1        v2        v3        v4        v5        v6        v7  \\\n",
       "img1   -0.290258 -0.260766  0.398556  0.551459  0.203292  0.350700  0.090883   \n",
       "img2   -0.231810 -0.086052 -0.523469 -0.638392 -0.316659  0.054627 -0.208353   \n",
       "img3    0.167813  0.094185  0.182432 -0.162009 -0.086838  0.256734 -0.065228   \n",
       "img4    0.179203  0.060119 -0.010540  0.228525  0.077062  0.064330  0.343766   \n",
       "img5    0.304406  0.443552  0.005249  0.065611 -0.039035  0.138633  0.044665   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "img980 -0.049621 -0.194991 -0.377691 -0.117684 -0.192404 -0.226072 -0.064908   \n",
       "img981 -0.051303 -0.079236 -0.621526 -0.791669 -0.165708 -0.275625 -0.062718   \n",
       "img982  0.026921 -0.110675 -1.328202 -0.824288 -0.489548 -0.511362  0.156399   \n",
       "img983  0.154602  0.323428  0.574714  0.690337  0.180027 -0.063628  0.121437   \n",
       "img984 -0.056947  0.232762 -0.199307  0.151706 -0.037999 -0.041623 -0.241919   \n",
       "\n",
       "              v8        v9       v10  ...    v18995    v18996    v18997  \\\n",
       "img1    0.560466  0.076039 -0.470655  ... -0.197475 -0.260196 -0.135420   \n",
       "img2   -0.894260 -0.325251 -0.335083  ...  0.265778  0.319153  0.250359   \n",
       "img3   -0.337406 -0.042966 -0.421225  ...  0.337032  0.353427  0.318367   \n",
       "img4    0.627719  0.007319 -0.000853  ...  0.059600  0.089175  0.078179   \n",
       "img5   -0.205013 -0.339522  0.008705  ...  0.382797  0.463299  0.298858   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "img980 -0.176825 -0.045374 -0.143872  ... -0.037360 -0.060519 -0.017378   \n",
       "img981 -0.615584  0.181013  0.181578  ...  0.062214  0.054557  0.067791   \n",
       "img982 -1.129562  0.256481  0.513830  ... -0.255496 -0.145004 -0.229646   \n",
       "img983  0.386864 -0.322702 -0.039456  ...  0.020442  0.034058  0.000138   \n",
       "img984  0.001214 -0.040044 -0.435984  ...  0.113146  0.108532  0.146041   \n",
       "\n",
       "          v18998    v18999    v19000    v19001    v19002    v19003    v19004  \n",
       "img1   -0.090778 -0.017567 -0.017567 -0.084397 -0.254089 -0.320621 -0.044061  \n",
       "img2    0.200973  0.152748  0.152748  0.009740  0.038726  0.053515  0.055127  \n",
       "img3    0.299446  0.242927  0.242927  0.316172  0.378080  0.363827  0.152742  \n",
       "img4    0.075996  0.071723  0.071723 -0.008810  0.030115  0.040529  0.010019  \n",
       "img5    0.198986  0.101586  0.101586  0.114196  0.156807  0.192245 -0.022523  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "img980 -0.001417  0.031624  0.031624 -0.009529  0.005759  0.002183  0.072564  \n",
       "img981  0.023669 -0.030190 -0.030190 -0.025124 -0.154573 -0.176024 -0.111984  \n",
       "img982 -0.270375 -0.281994 -0.281994 -0.473573 -0.484659 -0.413436 -0.319208  \n",
       "img983 -0.028001 -0.007571 -0.007571  0.134924  0.232575  0.249692 -0.005120  \n",
       "img984  0.188329  0.158098  0.158098  0.228462  0.271481  0.256635  0.177602  \n",
       "\n",
       "[984 rows x 19004 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "row_labels = ['img'+str(i+1) for i in range(lh_fmri_val_pred.shape[0])]\n",
    "col_labels = ['v'+str(i+1) for i in range(lh_fmri_val_pred.shape[1])]\n",
    "\n",
    "# Creazione del DataFrame\n",
    "fmri_val_pred_df = pd.DataFrame(lh_fmri_val_pred, index=row_labels, columns=col_labels)\n",
    "fmri_val_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30821148 0.36844289 0.58986917 0.55375606 0.25530387] Shape:  (19004,)\n"
     ]
    }
   ],
   "source": [
    "print(lh_correlation[:5], \"Shape: \", lh_correlation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19004,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_correlation.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare submission files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to convert your predicted fMRI responses to `float32` data type. This is important, since the maximum submission size is 300MB, and a `float64` data type would exceed this limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_fmri_val_pred = lh_fmri_val_pred.astype(np.float32)\n",
    "rh_fmri_val_pred = rh_fmri_val_pred.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the predicted fMRI responses to the test images in the subject-specific submission directory that we created previously (`../algonauts_2023_challenge_submission/subj0X/`), where `'X'` goes from `'1'` to `'8'`), with separate files for the predictions of each hemisphere: `lh_pred_test.npy` for the left hemisphere and `rh_pred_test.npy` for the right hemisphere.\n",
    "\n",
    "<font color='red'><b>NOTE:</b></font> these files need to contain the predicted brain responses of all vertices, and not of any ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(args.subject_submission_dir, 'lh_pred_val.npy'), lh_fmri_val_pred)\n",
    "np.save(os.path.join(args.subject_submission_dir, 'rh_pred_val.npy'), rh_fmri_val_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, choose which of the 8 subjects you will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 6 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some paths that we will need for loading and storing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "    # Define the dir where data is stored\n",
    "    # 1 became 01\n",
    "    self.subj = format(subj, '02') # '0numberofchars'\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:28<00:00,  3.59s/it]\n"
     ]
    }
   ],
   "source": [
    "correlation = np.empty(0)\n",
    "for subj in tqdm(range(1, 9), total=len(range(1, 9))):\n",
    "    args = argObj(data_dir, parent_submission_dir, subj)\n",
    "    \n",
    "    train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "    train_img_list = os.listdir(train_img_dir)\n",
    "    train_img_list.sort()\n",
    "    \n",
    "    rand_seed = 5 #@param\n",
    "    np.random.seed(rand_seed)\n",
    "\n",
    "    # Calculate how many stimulus images correspond to 90% of the training data\n",
    "    num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "    # Shuffle all training stimulus images\n",
    "    idxs = np.arange(len(train_img_list))\n",
    "    np.random.shuffle(idxs)\n",
    "    # Assign 90% of the shuffled stimulus images to the training partition,\n",
    "    # and 10% to the test partition\n",
    "    idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "    \n",
    "    fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "    lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "    rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "    lh_fmri = lh_fmri[idxs_val]\n",
    "    rh_fmri = rh_fmri[idxs_val]\n",
    "    \n",
    "    lh_pred_fmri = np.load(os.path.join(args.subject_submission_dir,\"lh_pred_val.npy\"))\n",
    "    rh_pred_fmri = np.load(os.path.join(args.subject_submission_dir,\"rh_pred_val.npy\"))\n",
    "\n",
    "    # print(f'Subj{subj} - LH training fMRI data shape:')\n",
    "    # print(lh_fmri.shape)\n",
    "    # print(lh_pred_fmri.shape)\n",
    "    # print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "    # print(f'Subj{subj} - \\nRH training fMRI data shape:')\n",
    "    # print(rh_fmri.shape)\n",
    "    # print(rh_pred_fmri.shape)\n",
    "    # print('(Training stimulus images × RH vertices)')\n",
    "    \n",
    "    # Empty correlation array of shape: (LH vertices)\n",
    "    lh_correlation = np.zeros(lh_pred_fmri.shape[1])\n",
    "    # Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "    for v in range(lh_pred_fmri.shape[1]):\n",
    "        lh_correlation[v] = corr(lh_pred_fmri[:,v], lh_fmri[:,v])[0] # 0 per selezionare valore e non p-value\n",
    "\n",
    "    correlation = np.append(correlation, lh_correlation)\n",
    "    # Empty correlation array of shape: (RH vertices)\n",
    "    rh_correlation = np.zeros(rh_pred_fmri.shape[1])\n",
    "    # Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "    for v in range(rh_pred_fmri.shape[1]):\n",
    "        rh_correlation[v] = corr(rh_pred_fmri[:,v], rh_fmri[:,v])[0]\n",
    "    correlation = np.append(correlation, rh_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315997,)\n",
      "0.17844173747681108\n"
     ]
    }
   ],
   "source": [
    "print(correlation.shape)\n",
    "print(np.median(correlation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj in range(1, 9):\n",
    "    args = argObj(data_dir, parent_submission_dir, subj)\n",
    "    fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "    lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "    rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "    print(f'Subj{subj} - LH training fMRI data shape:')\n",
    "    print(lh_fmri.shape)\n",
    "    #print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "    print(f'Subj{subj} - \\nRH training fMRI data shape:')\n",
    "    print(rh_fmri.shape)\n",
    "    #print('(Training stimulus images × RH vertices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensioni della matrice lh per subj01 : (984, 19004)\n",
      "Dimensioni della matrice rh per subj01 : (984, 20544)\n",
      "Dimensioni della matrice lh per subj02 : (984, 19004)\n",
      "Dimensioni della matrice rh per subj02 : (984, 20544)\n",
      "Dimensioni della matrice lh per subj03 : (908, 19004)\n",
      "Dimensioni della matrice rh per subj03 : (908, 20544)\n",
      "Dimensioni della matrice lh per subj04 : (878, 19004)\n",
      "Dimensioni della matrice rh per subj04 : (878, 20544)\n",
      "Dimensioni della matrice lh per subj05 : (984, 19004)\n",
      "Dimensioni della matrice rh per subj05 : (984, 20544)\n",
      "Dimensioni della matrice lh per subj06 : (908, 18978)\n",
      "Dimensioni della matrice rh per subj06 : (908, 20220)\n",
      "Dimensioni della matrice lh per subj07 : (984, 19004)\n",
      "Dimensioni della matrice rh per subj07 : (984, 20544)\n",
      "Dimensioni della matrice lh per subj08 : (878, 18981)\n",
      "Dimensioni della matrice rh per subj08 : (878, 20530)\n"
     ]
    }
   ],
   "source": [
    "# Creo una lista con i nomi delle cartelle\n",
    "folders = [\"subj01\", \"subj02\", \"subj03\", \"subj04\", \"subj05\", \"subj06\", \"subj07\", \"subj08\"]\n",
    "# Itero sulla lista\n",
    "for folder in folders:\n",
    "  # Costruisco i percorsi dei file da importare\n",
    "  lh_file = parent_submission_dir + \"/\" + folder + \"/lh_pred_val.npy\"\n",
    "  rh_file = parent_submission_dir + \"/\" + folder + \"/rh_pred_val.npy\"\n",
    "  # Carico i file come matrici numpy\n",
    "  lh_matrix = np.load(lh_file)\n",
    "  rh_matrix = np.load(rh_file)\n",
    "  # Stampo le dimensioni delle matrici\n",
    "  print(\"Dimensioni della matrice lh per\", folder, \":\", np.shape(lh_matrix))\n",
    "  print(\"Dimensioni della matrice rh per\", folder, \":\", np.shape(rh_matrix))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are right, the tutorial only goes as far as calculating the Pearson's correlation between the predicted and ground truth fMRI responses. This is because the Challenge data release didn't include the ncsnr, which is necessary to compute the noise normalized encoding accuracy (the official Challenge evaluation metric).\n",
    "\n",
    "You can download the ncsnr from the AWS NSD data release (/natural-scenes-dataset/nsddata_betas/ppdata/subj0X/fsaverage/betas_fithrf_GLMdenoise_RR/Xh.ncsnr.mgh), and then index the vertices retained for the Challenge using the vertices mask we provide in the Challenge data release (/algonauts_2023_challenge_data/subj0X/roi_masks/Xh.all-vertices_fsaverage_space.npy)\n",
    "\n",
    "Once you have the ncsnr of the Challenge vertices, you will have to transform it into the noise ceiling based on how many trials exist (from 1 to 3) for each of the image conditions you will use to evaluate your models (i.e., the images of the Challenge train split on which you will evaluate your models). However, you cannot infer the trial number of each image condition from the Challenge data, as this data is averaged across trials of each image condition. Here you will find a vector variable for each Challenge subject indicating how many trials (from 1 to 3) were averaged to obtain the fMRI responses for each image condition of the Challenge train split. For the transformation from ncsnr to noise ceiling, please use the last equation of the \"Noise ceiling\" paragraph in the \"Functional data (NSD)\" section of the NSD data manual.\n",
    "\n",
    "Finally, once you have the noise ceiling, you can use it to calculate the noise normalized encoding accuracy (the code attached to this email is what we use to compute the Challenge evaluation metric).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing ncsnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nibabel.freesurfer.mghformat import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_ncsnr = load(os.path.join(args.ncsnr_dir, 'lh.ncsnr.mgh'))\n",
    "rh_ncsnr = load(os.path.join(args.ncsnr_dir, 'rh.ncsnr.mgh'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the ncsnr numpy 1D vector from the MGHImage object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_ncsnr_all_vertices = lh_ncsnr.get_fdata()[:,0,0]\n",
    "rh_ncsnr_all_vertices = rh_ncsnr.get_fdata()[:,0,0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nibabel.freesurfer.mghformat.MGHImage"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lh_ncsnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163842, 1, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_ncsnr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163842,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_ncsnr_all_vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11920288 0.31043354 0.11843289 ... 0.03596502 0.03923279 0.11306294] shape (163842,)\n"
     ]
    }
   ],
   "source": [
    "print(lh_ncsnr.get_fdata()[:,0,0], \"shape\", lh_ncsnr.get_fdata()[:,0,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(lh_ncsnr_all_vertices, bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing fsaverage \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Xh.all-vertices_fsaverage_space.npy` is a vertices mask that associate to every of the 163842 vertices of the `fsaverage_space` a value of 1 if it is part of the `challenge space` and 0 if it is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere = ['left', 'right'] #@param ['left', 'right'] {allow-input: true}\n",
    "\n",
    "# Load the brain surface map of all vertices\n",
    "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0][0]+'h.all-vertices_fsaverage_space.npy')\n",
    "lh_fsaverage_all_vertices = np.load(roi_dir)\n",
    "\n",
    "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[1][0]+'h.all-vertices_fsaverage_space.npy')\n",
    "rh_fsaverage_all_vertices = np.load(roi_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163842,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_fsaverage_all_vertices.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to extract indicies of vertices in the challenge space from the fsaverage space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     6,      7,     32, ..., 163061, 163062, 163063], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lh_fsaverage_all_vertices)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19004"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(lh_fsaverage_all_vertices)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fsaverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['area_left', 'area_right', 'curv_left', 'curv_right', 'infl_left', 'infl_right', 'pial_left', 'pial_right', 'sphere_left', 'sphere_right', 'sulc_left', 'sulc_right', 'thick_left', 'thick_right', 'white_left', 'white_right', 'description'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsaverage.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract ncsnr for vertices in the challenge space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_ncsnr_challenge_vertices = lh_ncsnr_all_vertices[np.where(lh_fsaverage_all_vertices)[0]]\n",
    "rh_ncsnr_challenge_vertices = rh_ncsnr_all_vertices[np.where(rh_fsaverage_all_vertices)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45753199 0.30325422 0.69066209 1.016029   0.27059516] shape (19004,)\n"
     ]
    }
   ],
   "source": [
    "print(lh_ncsnr_challenge_vertices[:5], \"shape\", lh_ncsnr_challenge_vertices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.501568849998995"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lh_ncsnr_challenge_vertices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import images number of trial (needed to compute Noise Ceiling) for the ENTIRE training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_trial_number = np.load(os.path.join(args.images_trials_dir, 'train_images_trials.npy'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 3 3 3] shape (9841,)\n"
     ]
    }
   ],
   "source": [
    "print(image_trial_number, \"shape\", image_trial_number.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9841,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_trial_number.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(image_trial_number)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(image_trial_number[idxs_val])\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the number of trials for the validation set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_trial_number_val = image_trial_number[idxs_val]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Noise Ceilings (NC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining:\n",
    "- A = number of images with 3 trials\n",
    "- B = number of images with 2 trials\n",
    "- C = numebr of images with 1 trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834 117 33\n"
     ]
    }
   ],
   "source": [
    "A = len(image_trial_number_val[image_trial_number_val == 3])\n",
    "B = len(image_trial_number_val[image_trial_number_val == 2])\n",
    "C = len(image_trial_number_val[image_trial_number_val == 1])\n",
    "\n",
    "print(A, B, C)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Noice Ceiling\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_noise_ceiling = (lh_ncsnr_challenge_vertices ** 2) / ((lh_ncsnr_challenge_vertices ** 2) + ((A/3 + B/2 + C/1) / (A + B + C)))\n",
    "rh_noise_ceiling = (rh_ncsnr_challenge_vertices ** 2) / ((rh_ncsnr_challenge_vertices ** 2) + ((A/3 + B/2 + C/1) / (A + B + C)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NC exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45753199 0.30325422 0.69066209 1.016029   0.27059516] shape (19004,)\n"
     ]
    }
   ],
   "source": [
    "print(lh_ncsnr_challenge_vertices[:5], \"shape\", lh_noise_ceiling.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38257236 0.21396401 0.58539575 0.75342749 0.17812687] shape (19004,)\n"
     ]
    }
   ],
   "source": [
    "print(lh_noise_ceiling[:5], \"shape\", lh_noise_ceiling.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3814481862765162"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lh_noise_ceiling)\n",
    "np.mean(lh_noise_ceiling)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute NOISE NORMALIZED SQUARED CORRELATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set negative correlation values to 0, so to keep the noise-normalized encoding accuracy positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_correlation[lh_correlation<0] = 0\n",
    "rh_correlation[rh_correlation<0] = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Square the correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_correlation = lh_correlation ** 2\n",
    "rh_correlation = rh_correlation ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09499432 0.13575016 0.34794564 0.30664577 0.06518006] shape (19004,)\n"
     ]
    }
   ],
   "source": [
    "print(lh_correlation[:5], \"shape\", lh_correlation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a very small number to noise ceiling values of 0, otherwise the noise-normalized encoding accuracy cannot be calculated (division by 0 is not possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_noise_ceiling[lh_noise_ceiling==0] = 1e-14\n",
    "rh_noise_ceiling[rh_noise_ceiling==0] = 1e-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35793416 0.19672465 0.55953275 0.73327037 0.16317555] shape (19004,)\n"
     ]
    }
   ],
   "source": [
    "print(lh_noise_ceiling[:5], \"shape\", lh_noise_ceiling.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the noise-normalized encoding accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_noise_norm_corr = np.divide(lh_correlation, lh_noise_ceiling)\n",
    "rh_noise_norm_corr = np.divide(rh_correlation, rh_noise_ceiling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26539606 0.6900516  0.62185036 0.41818923 0.3994475 ] shape (19004,)\n"
     ]
    }
   ],
   "source": [
    "print(lh_noise_norm_corr[:5], \"shape\", lh_noise_norm_corr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the noise-normalized encoding accuracy to 1 (100% accuracy) for those vertices in which the correlation is higher than the noise ceiling, to prevent encoding accuracy values higher than 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_noise_norm_corr[lh_noise_norm_corr>1] = 1\n",
    "rh_noise_norm_corr[rh_noise_norm_corr>1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute **median noise normalized squared correlation (in this EXAMPLE only for the singles emispheres)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1407182913703659\n",
      "0.1262181866791165\n",
      "0.18697322868340183\n",
      "0.1670923019675932\n"
     ]
    }
   ],
   "source": [
    "print(np.median(lh_noise_norm_corr))\n",
    "print(np.median(rh_noise_norm_corr))\n",
    "print(np.mean(lh_noise_norm_corr))\n",
    "print(np.mean(rh_noise_norm_corr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject -> lh_3 | Median Noise Normalized Squared Correlation Percentage -> 14.07182913703659\n",
      "Subject -> rh_3 | Median Noise Normalized Squared Correlation Percentage -> 12.621818667911649\n"
     ]
    }
   ],
   "source": [
    "noise_norm_corr_dict = {}\n",
    "noise_norm_corr_dict[f'lh_{subj}'] = lh_noise_norm_corr\n",
    "noise_norm_corr_dict[f'rh_{subj}'] = rh_noise_norm_corr\n",
    "for key, value in noise_norm_corr_dict.items():\n",
    "    print(\"Subject ->\",key,\"| Median Noise Normalized Squared Correlation Percentage ->\",np.median(value)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1330357811214621\n"
     ]
    }
   ],
   "source": [
    "concatenated_array = np.concatenate(list(noise_norm_corr_dict.values()))\n",
    "median = np.median(concatenated_array)\n",
    "\n",
    "print(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 9)\n"
     ]
    }
   ],
   "source": [
    "print(range(1, 9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algonauts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
